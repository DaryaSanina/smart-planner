{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e802d67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Task</th>\n",
       "      <th>Importance</th>\n",
       "      <th>Time to complete (minutes)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prepare presentation for client meeting</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pay medical bills</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Submit tax forms</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Schedule dentist appointment</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Register for training session</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                     Task  Importance  \\\n",
       "0           0  Prepare presentation for client meeting          10   \n",
       "1           1                        Pay medical bills           6   \n",
       "2           2                         Submit tax forms           5   \n",
       "3           3             Schedule dentist appointment           6   \n",
       "4           4            Register for training session           7   \n",
       "\n",
       "   Time to complete (minutes)  \n",
       "0                          90  \n",
       "1                          30  \n",
       "2                          30  \n",
       "3                          30  \n",
       "4                          45  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv('dataset.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953322ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "raw_tasks = dataframe[\"Task\"]\n",
    "importances = dataframe[\"Importance\"]\n",
    "print(len(raw_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20124e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['prepare', 'presentation', 'for', 'client', 'meeting'], ['pay', 'medical', 'bills'], ['submit', 'tax', 'forms'], ['schedule', 'dentist', 'appointment'], ['register', 'for', 'training', 'session']]\n"
     ]
    }
   ],
   "source": [
    "tokens = [[token for token in task.lower().split()] for task in raw_tasks]\n",
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65754bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for task in tokens:\n",
    "    for token in task:\n",
    "        vocabulary.add(token)\n",
    "vocabulary = list(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86fce95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 468,  374,  773,  146, 1478]), array([ 869,  618, 1248]), array([ 847,  656, 1240]), array([992, 967, 554]), array([913, 773, 949, 831])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2index = dict()\n",
    "for i, w in enumerate(vocabulary):\n",
    "    word2index[w] = i\n",
    "indices = [np.asarray([word2index[token] for token in task]) for task in tokens]\n",
    "print(indices[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7465857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import Tensor\n",
    "from layers import Embedding, LSTMCell, MSELoss, Linear\n",
    "from optimisers import SGD\n",
    "\n",
    "embedding = Embedding(vocab_size=len(vocabulary), dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_outputs=1)\n",
    "output_layer = Linear(n_inputs=512, n_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea42af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MSELoss()\n",
    "optimiser = SGD(parameters=model.get_parameters() + embedding.get_parameters() + output_layer.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a357ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "X_train = indices[:int(len(indices) * 0.8)]\n",
    "y_train = importances[:int(len(indices) * 0.8)]\n",
    "\n",
    "X_test = indices[int(len(indices) * 0.8):]\n",
    "y_test = importances[int(len(indices) * 0.8):]\n",
    "\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75590e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha: 0.05 - Example 4/8000 - Min Loss: 1000 - Loss: [2185.6413197]1]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5/8000 - Min Loss: [619. - Loss: [619.50600201]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6/8000 - Min Loss: [236. - Loss: [236.20143532]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7/8000 - Min Loss: [109. - Loss: [109.50493138]\n",
      "Iter: 0 - Alpha: 0.05 - Example 8/8000 - Min Loss: [60.9 - Loss: [60.93181182]\n",
      "Iter: 0 - Alpha: 0.05 - Example 14/8000 - Min Loss: [56.0 - Loss: [66.48309715]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 15/8000 - Min Loss: [51.0 - Loss: [51.04762177]\n",
      "Iter: 0 - Alpha: 0.05 - Example 16/8000 - Min Loss: [42.1 - Loss: [42.18814785]\n",
      "Iter: 0 - Alpha: 0.05 - Example 17/8000 - Min Loss: [34.2 - Loss: [34.24174177]\n",
      "Iter: 0 - Alpha: 0.05 - Example 18/8000 - Min Loss: [29.6 - Loss: [29.69959271]\n",
      "Iter: 0 - Alpha: 0.05 - Example 19/8000 - Min Loss: [24.8 - Loss: [24.86774854]\n",
      "Iter: 0 - Alpha: 0.05 - Example 20/8000 - Min Loss: [21.1 - Loss: [21.1789909]\n",
      "Iter: 0 - Alpha: 0.05 - Example 21/8000 - Min Loss: [20.7 - Loss: [20.73716392]\n",
      "Iter: 0 - Alpha: 0.05 - Example 23/8000 - Min Loss: [19.3 - Loss: [20.06994805]\n",
      "Iter: 0 - Alpha: 0.05 - Example 24/8000 - Min Loss: [18.1 - Loss: [18.12485789]\n",
      "Iter: 0 - Alpha: 0.05 - Example 25/8000 - Min Loss: [16.2 - Loss: [16.25000947]\n",
      "Iter: 0 - Alpha: 0.05 - Example 26/8000 - Min Loss: [14.7 - Loss: [14.75254734]\n",
      "Iter: 0 - Alpha: 0.05 - Example 27/8000 - Min Loss: [14.0 - Loss: [14.08013898]\n",
      "Iter: 0 - Alpha: 0.05 - Example 34/8000 - Min Loss: [12.9 - Loss: [13.0087843]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 35/8000 - Min Loss: [12.2 - Loss: [12.22226649]\n",
      "Iter: 0 - Alpha: 0.05 - Example 36/8000 - Min Loss: [11.4 - Loss: [11.42493952]\n",
      "Iter: 0 - Alpha: 0.05 - Example 39/8000 - Min Loss: [10.9 - Loss: [11.24402578]\n",
      "Iter: 0 - Alpha: 0.05 - Example 42/8000 - Min Loss: [10.5 - Loss: [10.78208636]\n",
      "Iter: 0 - Alpha: 0.05 - Example 43/8000 - Min Loss: [10.2 - Loss: [10.20487679]\n",
      "Iter: 0 - Alpha: 0.05 - Example 44/8000 - Min Loss: [9.71 - Loss: [9.7121473]\n",
      "Iter: 0 - Alpha: 0.05 - Example 45/8000 - Min Loss: [9.37 - Loss: [9.37331967]\n",
      "Iter: 0 - Alpha: 0.05 - Example 46/8000 - Min Loss: [8.97 - Loss: [8.97813348]\n",
      "Iter: 0 - Alpha: 0.05 - Example 50/8000 - Min Loss: [8.94 - Loss: [8.98292597]\n",
      "Iter: 0 - Alpha: 0.05 - Example 51/8000 - Min Loss: [8.60 - Loss: [8.6093904]\n",
      "Iter: 0 - Alpha: 0.05 - Example 52/8000 - Min Loss: [8.37 - Loss: [8.37074188]\n",
      "Iter: 0 - Alpha: 0.05 - Example 59/8000 - Min Loss: [8.08 - Loss: [8.3422825]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 60/8000 - Min Loss: [8.05 - Loss: [8.05252352]\n",
      "Iter: 0 - Alpha: 0.05 - Example 61/8000 - Min Loss: [7.78 - Loss: [7.78181225]\n",
      "Iter: 0 - Alpha: 0.05 - Example 62/8000 - Min Loss: [7.52 - Loss: [7.52933622]\n",
      "Iter: 0 - Alpha: 0.05 - Example 63/8000 - Min Loss: [7.29 - Loss: [7.29208736]\n",
      "Iter: 0 - Alpha: 0.05 - Example 64/8000 - Min Loss: [7.09 - Loss: [7.09635218]\n",
      "Iter: 0 - Alpha: 0.05 - Example 65/8000 - Min Loss: [6.98 - Loss: [6.98590503]\n",
      "Iter: 0 - Alpha: 0.05 - Example 66/8000 - Min Loss: [6.81 - Loss: [6.81551417]\n",
      "Iter: 0 - Alpha: 0.05 - Example 67/8000 - Min Loss: [6.62 - Loss: [6.62843961]\n",
      "Iter: 0 - Alpha: 0.05 - Example 68/8000 - Min Loss: [6.50 - Loss: [6.50490671]\n",
      "Iter: 0 - Alpha: 0.05 - Example 69/8000 - Min Loss: [6.36 - Loss: [6.3682754]\n",
      "Iter: 0 - Alpha: 0.05 - Example 70/8000 - Min Loss: [6.35 - Loss: [6.35278843]\n",
      "Iter: 0 - Alpha: 0.05 - Example 71/8000 - Min Loss: [6.19 - Loss: [6.19693037]\n",
      "Iter: 0 - Alpha: 0.05 - Example 72/8000 - Min Loss: [6.04 - Loss: [6.0452267]\n",
      "Iter: 0 - Alpha: 0.05 - Example 73/8000 - Min Loss: [5.98 - Loss: [5.98816497]\n",
      "Iter: 0 - Alpha: 0.05 - Example 77/8000 - Min Loss: [5.84 - Loss: [5.95641216]\n",
      "Iter: 0 - Alpha: 0.05 - Example 78/8000 - Min Loss: [5.83 - Loss: [5.83347515]\n",
      "Iter: 0 - Alpha: 0.05 - Example 79/8000 - Min Loss: [5.70 - Loss: [5.70943834]\n",
      "Iter: 0 - Alpha: 0.05 - Example 80/8000 - Min Loss: [5.65 - Loss: [5.65065307]\n",
      "Iter: 0 - Alpha: 0.05 - Example 81/8000 - Min Loss: [5.54 - Loss: [5.54784575]\n",
      "Iter: 0 - Alpha: 0.05 - Example 82/8000 - Min Loss: [5.43 - Loss: [5.43894639]\n",
      "Iter: 0 - Alpha: 0.05 - Example 94/8000 - Min Loss: [5.37 - Loss: [5.45343045]\n",
      "Iter: 0 - Alpha: 0.05 - Example 104/8000 - Min Loss: [5.35 - Loss: [5.36090552]\n",
      "Iter: 0 - Alpha: 0.05 - Example 105/8000 - Min Loss: [5.28 - Loss: [5.28004987]\n",
      "Iter: 0 - Alpha: 0.05 - Example 106/8000 - Min Loss: [5.22 - Loss: [5.22851597]\n",
      "Iter: 0 - Alpha: 0.05 - Example 107/8000 - Min Loss: [5.14 - Loss: [5.14940153]\n",
      "Iter: 0 - Alpha: 0.05 - Example 108/8000 - Min Loss: [5.12 - Loss: [5.12334776]\n",
      "Iter: 0 - Alpha: 0.05 - Example 109/8000 - Min Loss: [5.04 - Loss: [5.04720308]\n",
      "Iter: 0 - Alpha: 0.05 - Example 110/8000 - Min Loss: [4.97 - Loss: [4.97505704]\n",
      "Iter: 0 - Alpha: 0.05 - Example 112/8000 - Min Loss: [4.92 - Loss: [4.94681428]\n",
      "Iter: 0 - Alpha: 0.05 - Example 113/8000 - Min Loss: [4.88 - Loss: [4.88982511]\n",
      "Iter: 0 - Alpha: 0.05 - Example 114/8000 - Min Loss: [4.82 - Loss: [4.82603361]\n",
      "Iter: 0 - Alpha: 0.05 - Example 115/8000 - Min Loss: [4.81 - Loss: [4.81971995]\n",
      "Iter: 0 - Alpha: 0.05 - Example 116/8000 - Min Loss: [4.76 - Loss: [4.76919734]\n",
      "Iter: 0 - Alpha: 0.05 - Example 160/8000 - Min Loss: [4.71 - Loss: [4.72540236]\n",
      "Iter: 0 - Alpha: 0.05 - Example 162/8000 - Min Loss: [4.68 - Loss: [4.7224303]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 172/8000 - Min Loss: [4.67 - Loss: [4.70736481]\n",
      "Iter: 0 - Alpha: 0.05 - Example 174/8000 - Min Loss: [4.66 - Loss: [4.669906]2]\n",
      "Iter: 0 - Alpha: 0.05 - Example 175/8000 - Min Loss: [4.64 - Loss: [4.64273157]\n",
      "Iter: 0 - Alpha: 0.05 - Example 204/8000 - Min Loss: [4.63 - Loss: [4.64369385]\n",
      "Iter: 0 - Alpha: 0.05 - Example 205/8000 - Min Loss: [4.63 - Loss: [4.63154346]\n",
      "Iter: 0 - Alpha: 0.05 - Example 206/8000 - Min Loss: [4.60 - Loss: [4.60087067]\n",
      "Iter: 0 - Alpha: 0.05 - Example 207/8000 - Min Loss: [4.56 - Loss: [4.56875292]\n",
      "Iter: 0 - Alpha: 0.05 - Example 208/8000 - Min Loss: [4.56 - Loss: [4.56768852]\n",
      "Iter: 0 - Alpha: 0.05 - Example 209/8000 - Min Loss: [4.53 - Loss: [4.53740191]\n",
      "Iter: 0 - Alpha: 0.05 - Example 211/8000 - Min Loss: [4.51 - Loss: [4.52108582]\n",
      "Iter: 0 - Alpha: 0.05 - Example 212/8000 - Min Loss: [4.48 - Loss: [4.48911046]\n",
      "Iter: 0 - Alpha: 0.05 - Example 213/8000 - Min Loss: [4.47 - Loss: [4.47216246]\n",
      "Iter: 0 - Alpha: 0.05 - Example 214/8000 - Min Loss: [4.45 - Loss: [4.45162708]\n",
      "Iter: 0 - Alpha: 0.05 - Example 215/8000 - Min Loss: [4.42 - Loss: [4.42333865]\n",
      "Iter: 0 - Alpha: 0.05 - Example 216/8000 - Min Loss: [4.41 - Loss: [4.41063747]\n",
      "Iter: 0 - Alpha: 0.05 - Example 217/8000 - Min Loss: [4.38 - Loss: [4.38113402]\n",
      "Iter: 0 - Alpha: 0.05 - Example 218/8000 - Min Loss: [4.35 - Loss: [4.35322925]\n",
      "Iter: 0 - Alpha: 0.05 - Example 219/8000 - Min Loss: [4.33 - Loss: [4.33862409]\n",
      "Iter: 0 - Alpha: 0.05 - Example 220/8000 - Min Loss: [4.31 - Loss: [4.3168276]\n",
      "Iter: 0 - Alpha: 0.05 - Example 221/8000 - Min Loss: [4.28 - Loss: [4.2884919]\n",
      "Iter: 0 - Alpha: 0.05 - Example 222/8000 - Min Loss: [4.27 - Loss: [4.27392119]\n",
      "Iter: 0 - Alpha: 0.05 - Example 223/8000 - Min Loss: [4.25 - Loss: [4.25006018]\n",
      "Iter: 0 - Alpha: 0.05 - Example 224/8000 - Min Loss: [4.22 - Loss: [4.22797692]\n",
      "Iter: 0 - Alpha: 0.05 - Example 225/8000 - Min Loss: [4.20 - Loss: [4.20898117]\n",
      "Iter: 0 - Alpha: 0.05 - Example 256/8000 - Min Loss: [4.18 - Loss: [4.18554264]\n",
      "Iter: 0 - Alpha: 0.05 - Example 277/8000 - Min Loss: [4.17 - Loss: [4.17817831]\n",
      "Iter: 0 - Alpha: 0.05 - Example 278/8000 - Min Loss: [4.17 - Loss: [4.17134612]\n",
      "Iter: 0 - Alpha: 0.05 - Example 279/8000 - Min Loss: [4.16 - Loss: [4.1620757]\n",
      "Iter: 0 - Alpha: 0.05 - Example 280/8000 - Min Loss: [4.14 - Loss: [4.14963047]\n",
      "Iter: 0 - Alpha: 0.05 - Example 288/8000 - Min Loss: [4.13 - Loss: [4.13920401]\n",
      "Iter: 0 - Alpha: 0.05 - Example 289/8000 - Min Loss: [4.12 - Loss: [4.12041963]\n",
      "Iter: 0 - Alpha: 0.05 - Example 291/8000 - Min Loss: [4.11 - Loss: [4.12818075]\n",
      "Iter: 0 - Alpha: 0.05 - Example 293/8000 - Min Loss: [4.10 - Loss: [4.11043068]\n",
      "Iter: 0 - Alpha: 0.05 - Example 294/8000 - Min Loss: [4.09 - Loss: [4.09149568]\n",
      "Iter: 0 - Alpha: 0.05 - Example 296/8000 - Min Loss: [4.07 - Loss: [4.08402288]\n",
      "Iter: 0 - Alpha: 0.05 - Example 297/8000 - Min Loss: [4.06 - Loss: [4.06765574]\n",
      "Iter: 0 - Alpha: 0.05 - Example 301/8000 - Min Loss: [4.04 - Loss: [4.050962]6]\n",
      "Iter: 0 - Alpha: 0.05 - Example 302/8000 - Min Loss: [4.03 - Loss: [4.03329611]\n",
      "Iter: 0 - Alpha: 0.05 - Example 303/8000 - Min Loss: [4.02 - Loss: [4.02184168]\n",
      "Iter: 0 - Alpha: 0.05 - Example 304/8000 - Min Loss: [4.00 - Loss: [4.00871069]\n",
      "Iter: 0 - Alpha: 0.05 - Example 305/8000 - Min Loss: [3.99 - Loss: [3.99181078]\n",
      "Iter: 0 - Alpha: 0.05 - Example 306/8000 - Min Loss: [3.99 - Loss: [3.99043104]\n",
      "Iter: 0 - Alpha: 0.05 - Example 307/8000 - Min Loss: [3.97 - Loss: [3.97441908]\n",
      "Iter: 0 - Alpha: 0.05 - Example 308/8000 - Min Loss: [3.95 - Loss: [3.95769474]\n",
      "Iter: 0 - Alpha: 0.05 - Example 309/8000 - Min Loss: [3.94 - Loss: [3.94011867]\n",
      "Iter: 0 - Alpha: 0.05 - Example 310/8000 - Min Loss: [3.92 - Loss: [3.92272905]\n",
      "Iter: 0 - Alpha: 0.05 - Example 311/8000 - Min Loss: [3.90 - Loss: [3.9055283]\n",
      "Iter: 0 - Alpha: 0.05 - Example 312/8000 - Min Loss: [3.89 - Loss: [3.89322076]\n",
      "Iter: 0 - Alpha: 0.05 - Example 314/8000 - Min Loss: [3.87 - Loss: [3.88963097]\n",
      "Iter: 0 - Alpha: 0.05 - Example 315/8000 - Min Loss: [3.87 - Loss: [3.87509667]\n",
      "Iter: 0 - Alpha: 0.05 - Example 316/8000 - Min Loss: [3.86 - Loss: [3.86443039]\n",
      "Iter: 0 - Alpha: 0.05 - Example 317/8000 - Min Loss: [3.85 - Loss: [3.85426792]\n",
      "Iter: 0 - Alpha: 0.05 - Example 320/8000 - Min Loss: [3.84 - Loss: [3.8434815]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 323/8000 - Min Loss: [3.82 - Loss: [3.83935192]\n",
      "Iter: 0 - Alpha: 0.05 - Example 324/8000 - Min Loss: [3.82 - Loss: [3.82578702]\n",
      "Iter: 0 - Alpha: 0.05 - Example 326/8000 - Min Loss: [3.81 - Loss: [3.82111256]\n",
      "Iter: 0 - Alpha: 0.05 - Example 328/8000 - Min Loss: [3.80 - Loss: [3.82120512]\n",
      "Iter: 0 - Alpha: 0.05 - Example 329/8000 - Min Loss: [3.80 - Loss: [3.80624731]\n",
      "Iter: 0 - Alpha: 0.05 - Example 330/8000 - Min Loss: [3.79 - Loss: [3.79139554]\n",
      "Iter: 0 - Alpha: 0.05 - Example 331/8000 - Min Loss: [3.77 - Loss: [3.7781455]\n",
      "Iter: 0 - Alpha: 0.05 - Example 332/8000 - Min Loss: [3.77 - Loss: [3.77134987]\n",
      "Iter: 0 - Alpha: 0.05 - Example 333/8000 - Min Loss: [3.75 - Loss: [3.75662577]\n",
      "Iter: 0 - Alpha: 0.05 - Example 334/8000 - Min Loss: [3.74 - Loss: [3.74464891]\n",
      "Iter: 0 - Alpha: 0.05 - Example 335/8000 - Min Loss: [3.73 - Loss: [3.73118477]\n",
      "Iter: 0 - Alpha: 0.05 - Example 336/8000 - Min Loss: [3.72 - Loss: [3.72952899]\n",
      "Iter: 0 - Alpha: 0.05 - Example 342/8000 - Min Loss: [3.72 - Loss: [3.72767757]\n",
      "Iter: 0 - Alpha: 0.05 - Example 343/8000 - Min Loss: [3.71 - Loss: [3.71353958]\n",
      "Iter: 0 - Alpha: 0.05 - Example 344/8000 - Min Loss: [3.70 - Loss: [3.70263853]\n",
      "Iter: 0 - Alpha: 0.05 - Example 345/8000 - Min Loss: [3.69 - Loss: [3.69010379]\n",
      "Iter: 0 - Alpha: 0.05 - Example 346/8000 - Min Loss: [3.67 - Loss: [3.67724343]\n",
      "Iter: 0 - Alpha: 0.05 - Example 347/8000 - Min Loss: [3.66 - Loss: [3.66465978]\n",
      "Iter: 0 - Alpha: 0.05 - Example 352/8000 - Min Loss: [3.65 - Loss: [3.65354454]\n",
      "Iter: 0 - Alpha: 0.05 - Example 353/8000 - Min Loss: [3.64 - Loss: [3.64751448]\n",
      "Iter: 0 - Alpha: 0.05 - Example 354/8000 - Min Loss: [3.63 - Loss: [3.63420608]\n",
      "Iter: 0 - Alpha: 0.05 - Example 366/8000 - Min Loss: [3.62 - Loss: [3.63527812]\n",
      "Iter: 0 - Alpha: 0.05 - Example 368/8000 - Min Loss: [3.62 - Loss: [3.62702401]\n",
      "Iter: 0 - Alpha: 0.05 - Example 372/8000 - Min Loss: [3.61 - Loss: [3.62441167]\n",
      "Iter: 0 - Alpha: 0.05 - Example 373/8000 - Min Loss: [3.61 - Loss: [3.6119919]\n",
      "Iter: 0 - Alpha: 0.05 - Example 392/8000 - Min Loss: [3.59 - Loss: [3.60577697]\n",
      "Iter: 0 - Alpha: 0.05 - Example 394/8000 - Min Loss: [3.59 - Loss: [3.60452534]\n",
      "Iter: 0 - Alpha: 0.05 - Example 397/8000 - Min Loss: [3.59 - Loss: [3.60223585]\n",
      "Iter: 0 - Alpha: 0.05 - Example 400/8000 - Min Loss: [3.59 - Loss: [3.59236242]\n",
      "Iter: 0 - Alpha: 0.05 - Example 401/8000 - Min Loss: [3.58 - Loss: [3.58163925]\n",
      "Iter: 0 - Alpha: 0.05 - Example 431/8000 - Min Loss: [3.57 - Loss: [3.57790809]\n",
      "Iter: 0 - Alpha: 0.05 - Example 432/8000 - Min Loss: [3.56 - Loss: [3.56900766]\n",
      "Iter: 0 - Alpha: 0.05 - Example 434/8000 - Min Loss: [3.55 - Loss: [3.56557467]\n",
      "Iter: 0 - Alpha: 0.05 - Example 435/8000 - Min Loss: [3.55 - Loss: [3.5565698]\n",
      "Iter: 0 - Alpha: 0.05 - Example 437/8000 - Min Loss: [3.54 - Loss: [3.54926029]\n",
      "Iter: 0 - Alpha: 0.05 - Example 448/8000 - Min Loss: [3.54 - Loss: [3.54869171]\n",
      "Iter: 0 - Alpha: 0.05 - Example 449/8000 - Min Loss: [3.54 - Loss: [3.54226968]\n",
      "Iter: 0 - Alpha: 0.05 - Example 450/8000 - Min Loss: [3.53 - Loss: [3.53238373]\n",
      "Iter: 0 - Alpha: 0.05 - Example 451/8000 - Min Loss: [3.52 - Loss: [3.52517798]\n",
      "Iter: 0 - Alpha: 0.05 - Example 452/8000 - Min Loss: [3.51 - Loss: [3.51553316]\n",
      "Iter: 0 - Alpha: 0.05 - Example 454/8000 - Min Loss: [3.50 - Loss: [3.51443424]\n",
      "Iter: 0 - Alpha: 0.05 - Example 455/8000 - Min Loss: [3.50 - Loss: [3.50520869]\n",
      "Iter: 0 - Alpha: 0.05 - Example 456/8000 - Min Loss: [3.50 - Loss: [3.50335277]\n",
      "Iter: 0 - Alpha: 0.05 - Example 457/8000 - Min Loss: [3.50 - Loss: [3.50067472]\n",
      "Iter: 0 - Alpha: 0.05 - Example 458/8000 - Min Loss: [3.49 - Loss: [3.49828872]\n",
      "Iter: 0 - Alpha: 0.05 - Example 459/8000 - Min Loss: [3.48 - Loss: [3.48889195]\n",
      "Iter: 0 - Alpha: 0.05 - Example 461/8000 - Min Loss: [3.48 - Loss: [3.48090471]\n",
      "Iter: 0 - Alpha: 0.05 - Example 462/8000 - Min Loss: [3.47 - Loss: [3.47367996]\n",
      "Iter: 0 - Alpha: 0.05 - Example 481/8000 - Min Loss: [3.46 - Loss: [3.47415047]\n",
      "Iter: 0 - Alpha: 0.05 - Example 482/8000 - Min Loss: [3.46 - Loss: [3.46564713]\n",
      "Iter: 0 - Alpha: 0.05 - Example 483/8000 - Min Loss: [3.45 - Loss: [3.45941996]\n",
      "Iter: 0 - Alpha: 0.05 - Example 484/8000 - Min Loss: [3.45 - Loss: [3.45440956]\n",
      "Iter: 0 - Alpha: 0.05 - Example 495/8000 - Min Loss: [3.45 - Loss: [3.45677563]\n",
      "Iter: 0 - Alpha: 0.05 - Example 497/8000 - Min Loss: [3.45 - Loss: [3.45479606]\n",
      "Iter: 0 - Alpha: 0.05 - Example 498/8000 - Min Loss: [3.44 - Loss: [3.44676423]\n",
      "Iter: 0 - Alpha: 0.05 - Example 500/8000 - Min Loss: [3.43 - Loss: [3.44056429]\n",
      "Iter: 0 - Alpha: 0.05 - Example 502/8000 - Min Loss: [3.43 - Loss: [3.43776291]\n",
      "Iter: 0 - Alpha: 0.05 - Example 503/8000 - Min Loss: [3.42 - Loss: [3.42959814]\n",
      "Iter: 0 - Alpha: 0.05 - Example 504/8000 - Min Loss: [3.42 - Loss: [3.42414934]\n",
      "Iter: 0 - Alpha: 0.05 - Example 505/8000 - Min Loss: [3.41 - Loss: [3.41982367]\n",
      "Iter: 0 - Alpha: 0.05 - Example 508/8000 - Min Loss: [3.41 - Loss: [3.42046521]\n",
      "Iter: 0 - Alpha: 0.05 - Example 509/8000 - Min Loss: [3.41 - Loss: [3.41221134]\n",
      "Iter: 0 - Alpha: 0.05 - Example 510/8000 - Min Loss: [3.40 - Loss: [3.40729432]\n",
      "Iter: 0 - Alpha: 0.05 - Example 511/8000 - Min Loss: [3.39 - Loss: [3.3994077]\n",
      "Iter: 0 - Alpha: 0.05 - Example 512/8000 - Min Loss: [3.39 - Loss: [3.39228789]\n",
      "Iter: 0 - Alpha: 0.05 - Example 524/8000 - Min Loss: [3.38 - Loss: [3.39192576]\n",
      "Iter: 0 - Alpha: 0.05 - Example 525/8000 - Min Loss: [3.38 - Loss: [3.38498493]\n",
      "Iter: 0 - Alpha: 0.05 - Example 526/8000 - Min Loss: [3.37 - Loss: [3.37714715]\n",
      "Iter: 0 - Alpha: 0.05 - Example 528/8000 - Min Loss: [3.36 - Loss: [3.36970629]\n",
      "Iter: 0 - Alpha: 0.05 - Example 529/8000 - Min Loss: [3.36 - Loss: [3.36690004]\n",
      "Iter: 0 - Alpha: 0.05 - Example 530/8000 - Min Loss: [3.36 - Loss: [3.36624201]\n",
      "Iter: 0 - Alpha: 0.05 - Example 531/8000 - Min Loss: [3.35 - Loss: [3.35996854]\n",
      "Iter: 0 - Alpha: 0.05 - Example 532/8000 - Min Loss: [3.35 - Loss: [3.35461332]\n",
      "Iter: 0 - Alpha: 0.05 - Example 533/8000 - Min Loss: [3.34 - Loss: [3.34733171]\n",
      "Iter: 0 - Alpha: 0.05 - Example 537/8000 - Min Loss: [3.34 - Loss: [3.34556828]\n",
      "Iter: 0 - Alpha: 0.05 - Example 538/8000 - Min Loss: [3.33 - Loss: [3.33905342]\n",
      "Iter: 0 - Alpha: 0.05 - Example 539/8000 - Min Loss: [3.33 - Loss: [3.33206413]\n",
      "Iter: 0 - Alpha: 0.05 - Example 540/8000 - Min Loss: [3.32 - Loss: [3.32563372]\n",
      "Iter: 0 - Alpha: 0.05 - Example 541/8000 - Min Loss: [3.32 - Loss: [3.32038667]\n",
      "Iter: 0 - Alpha: 0.05 - Example 543/8000 - Min Loss: [3.31 - Loss: [3.31619014]\n",
      "Iter: 0 - Alpha: 0.05 - Example 544/8000 - Min Loss: [3.31 - Loss: [3.31240841]\n",
      "Iter: 0 - Alpha: 0.05 - Example 545/8000 - Min Loss: [3.31 - Loss: [3.31053327]\n",
      "Iter: 0 - Alpha: 0.05 - Example 546/8000 - Min Loss: [3.30 - Loss: [3.30333077]\n",
      "Iter: 0 - Alpha: 0.05 - Example 547/8000 - Min Loss: [3.30 - Loss: [3.30102288]\n",
      "Iter: 0 - Alpha: 0.05 - Example 548/8000 - Min Loss: [3.29 - Loss: [3.29552706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha: 0.05 - Example 549/8000 - Min Loss: [3.28 - Loss: [3.28996776]\n",
      "Iter: 0 - Alpha: 0.05 - Example 566/8000 - Min Loss: [3.28 - Loss: [3.28646226]\n",
      "Iter: 0 - Alpha: 0.05 - Example 567/8000 - Min Loss: [3.28 - Loss: [3.28116366]\n",
      "Iter: 0 - Alpha: 0.05 - Example 568/8000 - Min Loss: [3.28 - Loss: [3.28010162]\n",
      "Iter: 0 - Alpha: 0.05 - Example 570/8000 - Min Loss: [3.27 - Loss: [3.28108807]\n",
      "Iter: 0 - Alpha: 0.05 - Example 574/8000 - Min Loss: [3.27 - Loss: [3.27949252]\n",
      "Iter: 0 - Alpha: 0.05 - Example 575/8000 - Min Loss: [3.27 - Loss: [3.27287042]\n",
      "Iter: 0 - Alpha: 0.05 - Example 576/8000 - Min Loss: [3.26 - Loss: [3.26621118]\n",
      "Iter: 0 - Alpha: 0.05 - Example 577/8000 - Min Loss: [3.26 - Loss: [3.26247576]\n",
      "Iter: 0 - Alpha: 0.05 - Example 578/8000 - Min Loss: [3.25 - Loss: [3.25659041]\n",
      "Iter: 0 - Alpha: 0.05 - Example 582/8000 - Min Loss: [3.25 - Loss: [3.25239475]\n",
      "Iter: 0 - Alpha: 0.05 - Example 583/8000 - Min Loss: [3.24 - Loss: [3.24797214]\n",
      "Iter: 0 - Alpha: 0.05 - Example 584/8000 - Min Loss: [3.24 - Loss: [3.24795269]\n",
      "Iter: 0 - Alpha: 0.05 - Example 585/8000 - Min Loss: [3.24 - Loss: [3.24701422]\n",
      "Iter: 0 - Alpha: 0.05 - Example 586/8000 - Min Loss: [3.24 - Loss: [3.2466994]\n",
      "Iter: 0 - Alpha: 0.05 - Example 587/8000 - Min Loss: [3.24 - Loss: [3.24141673]\n",
      "Iter: 0 - Alpha: 0.05 - Example 603/8000 - Min Loss: [3.23 - Loss: [3.23655411]\n",
      "Iter: 0 - Alpha: 0.05 - Example 605/8000 - Min Loss: [3.23 - Loss: [3.23460444]\n",
      "Iter: 0 - Alpha: 0.05 - Example 628/8000 - Min Loss: [3.22 - Loss: [3.23448696]\n",
      "Iter: 0 - Alpha: 0.05 - Example 629/8000 - Min Loss: [3.22 - Loss: [3.2284575]\n",
      "Iter: 0 - Alpha: 0.05 - Example 630/8000 - Min Loss: [3.22 - Loss: [3.22461652]\n",
      "Iter: 0 - Alpha: 0.05 - Example 632/8000 - Min Loss: [3.22 - Loss: [3.2230368]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 685/8000 - Min Loss: [3.21 - Loss: [3.22201565]\n",
      "Iter: 0 - Alpha: 0.05 - Example 686/8000 - Min Loss: [3.21 - Loss: [3.21679385]\n",
      "Iter: 0 - Alpha: 0.05 - Example 687/8000 - Min Loss: [3.21 - Loss: [3.21354907]\n",
      "Iter: 0 - Alpha: 0.05 - Example 688/8000 - Min Loss: [3.20 - Loss: [3.2092335]\n",
      "Iter: 0 - Alpha: 0.05 - Example 709/8000 - Min Loss: [3.20 - Loss: [3.21066106]\n",
      "Iter: 0 - Alpha: 0.05 - Example 710/8000 - Min Loss: [3.20 - Loss: [3.20592745]\n",
      "Iter: 0 - Alpha: 0.05 - Example 712/8000 - Min Loss: [3.20 - Loss: [3.20248965]\n",
      "Iter: 0 - Alpha: 0.05 - Example 713/8000 - Min Loss: [3.19 - Loss: [3.19737154]\n",
      "Iter: 0 - Alpha: 0.05 - Example 714/8000 - Min Loss: [3.19 - Loss: [3.19348834]\n",
      "Iter: 0 - Alpha: 0.05 - Example 715/8000 - Min Loss: [3.19 - Loss: [3.19170985]\n",
      "Iter: 0 - Alpha: 0.05 - Example 717/8000 - Min Loss: [3.18 - Loss: [3.18701169]\n",
      "Iter: 0 - Alpha: 0.05 - Example 718/8000 - Min Loss: [3.18 - Loss: [3.18273765]\n",
      "Iter: 0 - Alpha: 0.05 - Example 719/8000 - Min Loss: [3.17 - Loss: [3.17875791]\n",
      "Iter: 0 - Alpha: 0.05 - Example 722/8000 - Min Loss: [3.17 - Loss: [3.1771161]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 723/8000 - Min Loss: [3.17 - Loss: [3.17280351]\n",
      "Iter: 0 - Alpha: 0.05 - Example 751/8000 - Min Loss: [3.16 - Loss: [3.17108098]\n",
      "Iter: 0 - Alpha: 0.05 - Example 752/8000 - Min Loss: [3.16 - Loss: [3.16651611]\n",
      "Iter: 0 - Alpha: 0.05 - Example 753/8000 - Min Loss: [3.16 - Loss: [3.16218542]\n",
      "Iter: 0 - Alpha: 0.05 - Example 754/8000 - Min Loss: [3.15 - Loss: [3.157402]\n",
      "Iter: 0 - Alpha: 0.05 - Example 756/8000 - Min Loss: [3.15 - Loss: [3.15433289]\n",
      "Iter: 0 - Alpha: 0.05 - Example 757/8000 - Min Loss: [3.15 - Loss: [3.15078394]\n",
      "Iter: 0 - Alpha: 0.05 - Example 758/8000 - Min Loss: [3.14 - Loss: [3.14750719]\n",
      "Iter: 0 - Alpha: 0.05 - Example 760/8000 - Min Loss: [3.14 - Loss: [3.14374089]\n",
      "Iter: 0 - Alpha: 0.05 - Example 761/8000 - Min Loss: [3.13 - Loss: [3.13921879]\n",
      "Iter: 0 - Alpha: 0.05 - Example 770/8000 - Min Loss: [3.13 - Loss: [3.13913841]\n",
      "Iter: 0 - Alpha: 0.05 - Example 792/8000 - Min Loss: [3.13 - Loss: [3.13637789]\n",
      "Iter: 0 - Alpha: 0.05 - Example 793/8000 - Min Loss: [3.13 - Loss: [3.13362992]\n",
      "Iter: 0 - Alpha: 0.05 - Example 850/8000 - Min Loss: [3.13 - Loss: [3.13114935]\n",
      "Iter: 0 - Alpha: 0.05 - Example 853/8000 - Min Loss: [3.12 - Loss: [3.13030527]\n",
      "Iter: 0 - Alpha: 0.05 - Example 856/8000 - Min Loss: [3.12 - Loss: [3.12783572]\n",
      "Iter: 0 - Alpha: 0.05 - Example 858/8000 - Min Loss: [3.12 - Loss: [3.12455042]\n",
      "Iter: 0 - Alpha: 0.05 - Example 859/8000 - Min Loss: [3.12 - Loss: [3.12229359]\n",
      "Iter: 0 - Alpha: 0.05 - Example 860/8000 - Min Loss: [3.11 - Loss: [3.1183456]\n",
      "Iter: 0 - Alpha: 0.05 - Example 861/8000 - Min Loss: [3.11 - Loss: [3.11587135]\n",
      "Iter: 0 - Alpha: 0.05 - Example 862/8000 - Min Loss: [3.11 - Loss: [3.11558524]\n",
      "Iter: 0 - Alpha: 0.05 - Example 863/8000 - Min Loss: [3.11 - Loss: [3.11209871]\n",
      "Iter: 0 - Alpha: 0.05 - Example 886/8000 - Min Loss: [3.10 - Loss: [3.11033605]\n",
      "Iter: 0 - Alpha: 0.05 - Example 914/8000 - Min Loss: [3.10 - Loss: [3.1090343]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 915/8000 - Min Loss: [3.10 - Loss: [3.10689381]\n",
      "Iter: 0 - Alpha: 0.05 - Example 920/8000 - Min Loss: [3.10 - Loss: [3.10565414]\n",
      "Iter: 0 - Alpha: 0.05 - Example 926/8000 - Min Loss: [3.10 - Loss: [3.10289514]\n",
      "Iter: 0 - Alpha: 0.05 - Example 927/8000 - Min Loss: [3.09 - Loss: [3.09936367]\n",
      "Iter: 0 - Alpha: 0.05 - Example 928/8000 - Min Loss: [3.09 - Loss: [3.09815796]\n",
      "Iter: 0 - Alpha: 0.05 - Example 929/8000 - Min Loss: [3.09 - Loss: [3.09455728]\n",
      "Iter: 0 - Alpha: 0.05 - Example 931/8000 - Min Loss: [3.09 - Loss: [3.09314867]\n",
      "Iter: 0 - Alpha: 0.05 - Example 945/8000 - Min Loss: [3.09 - Loss: [3.09221407]\n",
      "Iter: 0 - Alpha: 0.05 - Example 946/8000 - Min Loss: [3.08 - Loss: [3.0895837]\n",
      "Iter: 0 - Alpha: 0.05 - Example 947/8000 - Min Loss: [3.08 - Loss: [3.08733452]\n",
      "Iter: 0 - Alpha: 0.05 - Example 982/8000 - Min Loss: [3.08 - Loss: [3.08677787]\n",
      "Iter: 0 - Alpha: 0.05 - Example 983/8000 - Min Loss: [3.08 - Loss: [3.08325689]\n",
      "Iter: 0 - Alpha: 0.05 - Example 984/8000 - Min Loss: [3.08 - Loss: [3.08258854]\n",
      "Iter: 0 - Alpha: 0.05 - Example 985/8000 - Min Loss: [3.07 - Loss: [3.07909578]\n",
      "Iter: 0 - Alpha: 0.05 - Example 986/8000 - Min Loss: [3.07 - Loss: [3.07559513]\n",
      "Iter: 0 - Alpha: 0.05 - Example 989/8000 - Min Loss: [3.07 - Loss: [3.07474601]\n",
      "Iter: 0 - Alpha: 0.05 - Example 990/8000 - Min Loss: [3.07 - Loss: [3.07220384]\n",
      "Iter: 0 - Alpha: 0.05 - Example 991/8000 - Min Loss: [3.06 - Loss: [3.06987339]\n",
      "Iter: 0 - Alpha: 0.05 - Example 995/8000 - Min Loss: [3.06 - Loss: [3.07142904]\n",
      "Iter: 0 - Alpha: 0.05 - Example 996/8000 - Min Loss: [3.06 - Loss: [3.06859469]\n",
      "Iter: 0 - Alpha: 0.05 - Example 997/8000 - Min Loss: [3.06 - Loss: [3.06517847]\n",
      "Iter: 0 - Alpha: 0.05 - Example 998/8000 - Min Loss: [3.06 - Loss: [3.06174391]\n",
      "Iter: 0 - Alpha: 0.05 - Example 999/8000 - Min Loss: [3.05 - Loss: [3.05918522]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1000/8000 - Min Loss: [3.05 - Loss: [3.05752155]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1002/8000 - Min Loss: [3.05 - Loss: [3.05659975]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1004/8000 - Min Loss: [3.05 - Loss: [3.053683]7]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1010/8000 - Min Loss: [3.05 - Loss: [3.05109821]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1013/8000 - Min Loss: [3.04 - Loss: [3.0511983]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1051/8000 - Min Loss: [3.04 - Loss: [3.04955537]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1052/8000 - Min Loss: [3.04 - Loss: [3.04762638]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1053/8000 - Min Loss: [3.04 - Loss: [3.04551479]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1054/8000 - Min Loss: [3.04 - Loss: [3.04231921]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1055/8000 - Min Loss: [3.03 - Loss: [3.03914045]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1058/8000 - Min Loss: [3.03 - Loss: [3.03877353]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1060/8000 - Min Loss: [3.03 - Loss: [3.04001672]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1062/8000 - Min Loss: [3.03 - Loss: [3.03873406]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1063/8000 - Min Loss: [3.03 - Loss: [3.03573621]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1064/8000 - Min Loss: [3.03 - Loss: [3.03303145]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1065/8000 - Min Loss: [3.03 - Loss: [3.03088051]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1066/8000 - Min Loss: [3.02 - Loss: [3.02822912]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1067/8000 - Min Loss: [3.02 - Loss: [3.02579773]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1090/8000 - Min Loss: [3.02 - Loss: [3.02408671]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1091/8000 - Min Loss: [3.02 - Loss: [3.02123466]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1101/8000 - Min Loss: [3.01 - Loss: [3.02103806]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1102/8000 - Min Loss: [3.01 - Loss: [3.01804162]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1103/8000 - Min Loss: [3.01 - Loss: [3.01647049]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1104/8000 - Min Loss: [3.01 - Loss: [3.01350929]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1107/8000 - Min Loss: [3.01 - Loss: [3.01371644]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1108/8000 - Min Loss: [3.01 - Loss: [3.01072229]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1109/8000 - Min Loss: [3.00 - Loss: [3.00862764]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1110/8000 - Min Loss: [3.00 - Loss: [3.00593808]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1111/8000 - Min Loss: [3.00 - Loss: [3.0039506]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1115/8000 - Min Loss: [3.00 - Loss: [3.00387306]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1116/8000 - Min Loss: [3.00 - Loss: [3.00099824]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1117/8000 - Min Loss: [2.99 - Loss: [2.99914875]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1118/8000 - Min Loss: [2.99 - Loss: [2.99694242]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1119/8000 - Min Loss: [2.99 - Loss: [2.99561671]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1120/8000 - Min Loss: [2.99 - Loss: [2.99360589]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1124/8000 - Min Loss: [2.99 - Loss: [2.99233794]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1125/8000 - Min Loss: [2.99 - Loss: [2.9900957]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1126/8000 - Min Loss: [2.98 - Loss: [2.98929459]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1131/8000 - Min Loss: [2.98 - Loss: [2.98850676]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1134/8000 - Min Loss: [2.98 - Loss: [2.98713407]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1135/8000 - Min Loss: [2.98 - Loss: [2.98503864]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1136/8000 - Min Loss: [2.98 - Loss: [2.98243991]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1137/8000 - Min Loss: [2.97 - Loss: [2.97987354]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1138/8000 - Min Loss: [2.97 - Loss: [2.97710698]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1139/8000 - Min Loss: [2.97 - Loss: [2.97428002]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1140/8000 - Min Loss: [2.97 - Loss: [2.97190598]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1141/8000 - Min Loss: [2.96 - Loss: [2.96995759]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1143/8000 - Min Loss: [2.96 - Loss: [2.96987934]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1144/8000 - Min Loss: [2.96 - Loss: [2.9674544]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1145/8000 - Min Loss: [2.96 - Loss: [2.96476215]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1163/8000 - Min Loss: [2.96 - Loss: [2.96249232]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1164/8000 - Min Loss: [2.96 - Loss: [2.96010005]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1165/8000 - Min Loss: [2.95 - Loss: [2.95886916]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1166/8000 - Min Loss: [2.95 - Loss: [2.95614881]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1167/8000 - Min Loss: [2.95 - Loss: [2.95557193]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1168/8000 - Min Loss: [2.95 - Loss: [2.95326475]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1169/8000 - Min Loss: [2.95 - Loss: [2.95079948]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1171/8000 - Min Loss: [2.94 - Loss: [2.94895057]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1175/8000 - Min Loss: [2.94 - Loss: [2.94853062]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1236/8000 - Min Loss: [2.94 - Loss: [2.94682616]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1237/8000 - Min Loss: [2.94 - Loss: [2.94432465]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1238/8000 - Min Loss: [2.94 - Loss: [2.94195654]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1239/8000 - Min Loss: [2.93 - Loss: [2.9394014]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1241/8000 - Min Loss: [2.93 - Loss: [2.93755799]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1247/8000 - Min Loss: [2.93 - Loss: [2.93673554]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1248/8000 - Min Loss: [2.93 - Loss: [2.93435878]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1249/8000 - Min Loss: [2.93 - Loss: [2.93202513]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1250/8000 - Min Loss: [2.93 - Loss: [2.9309771]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1252/8000 - Min Loss: [2.92 - Loss: [2.9295574]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1253/8000 - Min Loss: [2.92 - Loss: [2.92787465]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1263/8000 - Min Loss: [2.92 - Loss: [2.92842384]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1264/8000 - Min Loss: [2.92 - Loss: [2.92607744]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1265/8000 - Min Loss: [2.92 - Loss: [2.92490588]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1266/8000 - Min Loss: [2.92 - Loss: [2.9226034]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1267/8000 - Min Loss: [2.92 - Loss: [2.92215617]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1270/8000 - Min Loss: [2.91 - Loss: [2.92029517]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1271/8000 - Min Loss: [2.91 - Loss: [2.91786211]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1272/8000 - Min Loss: [2.91 - Loss: [2.91662338]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1273/8000 - Min Loss: [2.91 - Loss: [2.91423551]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1328/8000 - Min Loss: [2.91 - Loss: [2.91242162]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1332/8000 - Min Loss: [2.91 - Loss: [2.91097238]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1333/8000 - Min Loss: [2.90 - Loss: [2.90949095]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1334/8000 - Min Loss: [2.90 - Loss: [2.90941366]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1335/8000 - Min Loss: [2.90 - Loss: [2.90767079]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1336/8000 - Min Loss: [2.90 - Loss: [2.90608094]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1337/8000 - Min Loss: [2.90 - Loss: [2.90499835]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1338/8000 - Min Loss: [2.90 - Loss: [2.90316478]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1339/8000 - Min Loss: [2.90 - Loss: [2.90136375]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1354/8000 - Min Loss: [2.90 - Loss: [2.90039235]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1356/8000 - Min Loss: [2.89 - Loss: [2.90083787]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1357/8000 - Min Loss: [2.89 - Loss: [2.89864042]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1358/8000 - Min Loss: [2.89 - Loss: [2.8963697]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1359/8000 - Min Loss: [2.89 - Loss: [2.89413217]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1363/8000 - Min Loss: [2.89 - Loss: [2.89397426]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1364/8000 - Min Loss: [2.89 - Loss: [2.89186912]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1365/8000 - Min Loss: [2.88 - Loss: [2.88988605]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1383/8000 - Min Loss: [2.88 - Loss: [2.89051021]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1384/8000 - Min Loss: [2.88 - Loss: [2.88855976]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1385/8000 - Min Loss: [2.88 - Loss: [2.88668221]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1386/8000 - Min Loss: [2.88 - Loss: [2.88451718]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1387/8000 - Min Loss: [2.88 - Loss: [2.88420037]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1388/8000 - Min Loss: [2.88 - Loss: [2.88201712]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1389/8000 - Min Loss: [2.88 - Loss: [2.88137769]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1390/8000 - Min Loss: [2.87 - Loss: [2.87923858]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1391/8000 - Min Loss: [2.87 - Loss: [2.87713198]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1392/8000 - Min Loss: [2.87 - Loss: [2.87531019]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1418/8000 - Min Loss: [2.87 - Loss: [2.87334693]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1422/8000 - Min Loss: [2.87 - Loss: [2.87271432]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1423/8000 - Min Loss: [2.87 - Loss: [2.87058481]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1424/8000 - Min Loss: [2.86 - Loss: [2.86886106]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1425/8000 - Min Loss: [2.86 - Loss: [2.86676945]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1426/8000 - Min Loss: [2.86 - Loss: [2.86496626]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1427/8000 - Min Loss: [2.86 - Loss: [2.86285556]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1428/8000 - Min Loss: [2.86 - Loss: [2.86145059]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1429/8000 - Min Loss: [2.85 - Loss: [2.85934806]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1430/8000 - Min Loss: [2.85 - Loss: [2.85784057]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1431/8000 - Min Loss: [2.85 - Loss: [2.8558223]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1439/8000 - Min Loss: [2.85 - Loss: [2.85567053]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1440/8000 - Min Loss: [2.85 - Loss: [2.85359173]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1441/8000 - Min Loss: [2.85 - Loss: [2.85172636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha: 0.05 - Example 1463/8000 - Min Loss: [2.84 - Loss: [2.85111644]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1475/8000 - Min Loss: [2.84 - Loss: [2.85069707]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1476/8000 - Min Loss: [2.84 - Loss: [2.84887984]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1477/8000 - Min Loss: [2.84 - Loss: [2.84691123]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1478/8000 - Min Loss: [2.84 - Loss: [2.84557434]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1485/8000 - Min Loss: [2.84 - Loss: [2.84535438]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1486/8000 - Min Loss: [2.84 - Loss: [2.84335318]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1497/8000 - Min Loss: [2.84 - Loss: [2.84178319]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1498/8000 - Min Loss: [2.83 - Loss: [2.83980271]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1499/8000 - Min Loss: [2.83 - Loss: [2.83783006]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1533/8000 - Min Loss: [2.83 - Loss: [2.83744177]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1534/8000 - Min Loss: [2.83 - Loss: [2.83556487]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1535/8000 - Min Loss: [2.83 - Loss: [2.83401572]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1536/8000 - Min Loss: [2.83 - Loss: [2.83384419]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1542/8000 - Min Loss: [2.83 - Loss: [2.83333571]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1544/8000 - Min Loss: [2.83 - Loss: [2.83328457]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1545/8000 - Min Loss: [2.83 - Loss: [2.83164894]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1546/8000 - Min Loss: [2.82 - Loss: [2.8297656]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1547/8000 - Min Loss: [2.82 - Loss: [2.82881227]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1548/8000 - Min Loss: [2.82 - Loss: [2.8279665]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1549/8000 - Min Loss: [2.82 - Loss: [2.82678948]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1550/8000 - Min Loss: [2.82 - Loss: [2.82490273]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1551/8000 - Min Loss: [2.82 - Loss: [2.82373804]\n",
      "Iter: 0 - Alpha: 0.05 - Example 1552/8000 - Min Loss: [2.82 - Loss: [2.82233127]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2306/8000 - Min Loss: [2.82 - Loss: [2.82176609]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2307/8000 - Min Loss: [2.82 - Loss: [2.82068281]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2308/8000 - Min Loss: [2.82 - Loss: [2.82047155]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2309/8000 - Min Loss: [2.82 - Loss: [2.82001059]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2310/8000 - Min Loss: [2.81 - Loss: [2.8196604]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2311/8000 - Min Loss: [2.81 - Loss: [2.81855383]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2312/8000 - Min Loss: [2.81 - Loss: [2.81845106]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2313/8000 - Min Loss: [2.81 - Loss: [2.81815841]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2314/8000 - Min Loss: [2.81 - Loss: [2.81689742]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2315/8000 - Min Loss: [2.81 - Loss: [2.81627764]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2316/8000 - Min Loss: [2.81 - Loss: [2.81501958]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2317/8000 - Min Loss: [2.81 - Loss: [2.81402405]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2318/8000 - Min Loss: [2.81 - Loss: [2.81396933]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2319/8000 - Min Loss: [2.81 - Loss: [2.81305492]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2320/8000 - Min Loss: [2.81 - Loss: [2.81231613]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2322/8000 - Min Loss: [2.81 - Loss: [2.8112448]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2323/8000 - Min Loss: [2.81 - Loss: [2.81063087]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2324/8000 - Min Loss: [2.80 - Loss: [2.80977675]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2325/8000 - Min Loss: [2.80 - Loss: [2.80884764]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2327/8000 - Min Loss: [2.80 - Loss: [2.80805058]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2328/8000 - Min Loss: [2.80 - Loss: [2.80683293]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2329/8000 - Min Loss: [2.80 - Loss: [2.80661325]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2330/8000 - Min Loss: [2.80 - Loss: [2.80540756]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2331/8000 - Min Loss: [2.80 - Loss: [2.80489781]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2332/8000 - Min Loss: [2.80 - Loss: [2.80482821]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2333/8000 - Min Loss: [2.80 - Loss: [2.80452044]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2337/8000 - Min Loss: [2.80 - Loss: [2.80453654]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2338/8000 - Min Loss: [2.80 - Loss: [2.80341229]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2339/8000 - Min Loss: [2.80 - Loss: [2.80263804]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2345/8000 - Min Loss: [2.80 - Loss: [2.80308541]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2347/8000 - Min Loss: [2.80 - Loss: [2.80202187]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2348/8000 - Min Loss: [2.80 - Loss: [2.80097867]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2349/8000 - Min Loss: [2.80 - Loss: [2.8003954]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2350/8000 - Min Loss: [2.80 - Loss: [2.80009989]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2351/8000 - Min Loss: [2.79 - Loss: [2.7988968]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2454/8000 - Min Loss: [2.79 - Loss: [2.79854265]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2457/8000 - Min Loss: [2.79 - Loss: [2.79803027]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2466/8000 - Min Loss: [2.79 - Loss: [2.79763598]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2467/8000 - Min Loss: [2.79 - Loss: [2.79672014]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2468/8000 - Min Loss: [2.79 - Loss: [2.79566066]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2513/8000 - Min Loss: [2.79 - Loss: [2.79558092]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2514/8000 - Min Loss: [2.79 - Loss: [2.79447204]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2517/8000 - Min Loss: [2.79 - Loss: [2.79359838]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2518/8000 - Min Loss: [2.79 - Loss: [2.79320592]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2710/8000 - Min Loss: [2.79 - Loss: [2.79236176]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2711/8000 - Min Loss: [2.79 - Loss: [2.79131555]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2712/8000 - Min Loss: [2.79 - Loss: [2.79090263]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2713/8000 - Min Loss: [2.78 - Loss: [2.78985305]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2721/8000 - Min Loss: [2.78 - Loss: [2.78933039]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2722/8000 - Min Loss: [2.78 - Loss: [2.78837004]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2723/8000 - Min Loss: [2.78 - Loss: [2.78735084]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2724/8000 - Min Loss: [2.78 - Loss: [2.78630211]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2725/8000 - Min Loss: [2.78 - Loss: [2.78525775]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2726/8000 - Min Loss: [2.78 - Loss: [2.78459496]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2727/8000 - Min Loss: [2.78 - Loss: [2.78391628]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2728/8000 - Min Loss: [2.78 - Loss: [2.78341265]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2729/8000 - Min Loss: [2.78 - Loss: [2.78278948]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2730/8000 - Min Loss: [2.78 - Loss: [2.7822076]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2731/8000 - Min Loss: [2.78 - Loss: [2.78120837]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2738/8000 - Min Loss: [2.78 - Loss: [2.78057212]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2739/8000 - Min Loss: [2.78 - Loss: [2.78024269]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2740/8000 - Min Loss: [2.77 - Loss: [2.7797935]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2742/8000 - Min Loss: [2.77 - Loss: [2.77951251]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2748/8000 - Min Loss: [2.77 - Loss: [2.77961931]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2749/8000 - Min Loss: [2.77 - Loss: [2.77862451]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2752/8000 - Min Loss: [2.77 - Loss: [2.77828591]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2755/8000 - Min Loss: [2.77 - Loss: [2.77764312]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2760/8000 - Min Loss: [2.77 - Loss: [2.77767155]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2761/8000 - Min Loss: [2.77 - Loss: [2.7766467]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2762/8000 - Min Loss: [2.77 - Loss: [2.77594779]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2802/8000 - Min Loss: [2.77 - Loss: [2.77671579]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2803/8000 - Min Loss: [2.77 - Loss: [2.77570432]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2804/8000 - Min Loss: [2.77 - Loss: [2.77478288]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2805/8000 - Min Loss: [2.77 - Loss: [2.77403293]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2806/8000 - Min Loss: [2.77 - Loss: [2.77329172]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2814/8000 - Min Loss: [2.77 - Loss: [2.77295141]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2816/8000 - Min Loss: [2.77 - Loss: [2.77207127]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2824/8000 - Min Loss: [2.77 - Loss: [2.77179662]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2833/8000 - Min Loss: [2.77 - Loss: [2.77152441]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2834/8000 - Min Loss: [2.77 - Loss: [2.77055355]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2861/8000 - Min Loss: [2.76 - Loss: [2.76989683]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2890/8000 - Min Loss: [2.76 - Loss: [2.76965109]\n",
      "Iter: 0 - Alpha: 0.05 - Example 2891/8000 - Min Loss: [2.76 - Loss: [2.76909871]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3468/8000 - Min Loss: [2.76 - Loss: [2.76906018]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3469/8000 - Min Loss: [2.76 - Loss: [2.76826176]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3470/8000 - Min Loss: [2.76 - Loss: [2.76750181]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3471/8000 - Min Loss: [2.76 - Loss: [2.76670071]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3472/8000 - Min Loss: [2.76 - Loss: [2.76589822]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3473/8000 - Min Loss: [2.76 - Loss: [2.76522442]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3474/8000 - Min Loss: [2.76 - Loss: [2.76512194]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3475/8000 - Min Loss: [2.76 - Loss: [2.76491362]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3478/8000 - Min Loss: [2.76 - Loss: [2.7641389]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3485/8000 - Min Loss: [2.76 - Loss: [2.76381885]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3490/8000 - Min Loss: [2.76 - Loss: [2.76332792]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3491/8000 - Min Loss: [2.76 - Loss: [2.76252348]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3505/8000 - Min Loss: [2.76 - Loss: [2.76240421]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3506/8000 - Min Loss: [2.76 - Loss: [2.76162004]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3507/8000 - Min Loss: [2.76 - Loss: [2.76106216]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3508/8000 - Min Loss: [2.76 - Loss: [2.76035416]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3509/8000 - Min Loss: [2.75 - Loss: [2.75962876]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3510/8000 - Min Loss: [2.75 - Loss: [2.75903946]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3511/8000 - Min Loss: [2.75 - Loss: [2.75872882]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3514/8000 - Min Loss: [2.75 - Loss: [2.75853214]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3520/8000 - Min Loss: [2.75 - Loss: [2.75814808]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3521/8000 - Min Loss: [2.75 - Loss: [2.757493]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3522/8000 - Min Loss: [2.75 - Loss: [2.75671657]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3523/8000 - Min Loss: [2.75 - Loss: [2.75608696]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3524/8000 - Min Loss: [2.75 - Loss: [2.75534767]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3525/8000 - Min Loss: [2.75 - Loss: [2.75470843]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3526/8000 - Min Loss: [2.75 - Loss: [2.75463296]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3527/8000 - Min Loss: [2.75 - Loss: [2.7545964]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3528/8000 - Min Loss: [2.75 - Loss: [2.75426275]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3530/8000 - Min Loss: [2.75 - Loss: [2.7539199]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3537/8000 - Min Loss: [2.75 - Loss: [2.75341739]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3555/8000 - Min Loss: [2.75 - Loss: [2.75311547]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3556/8000 - Min Loss: [2.75 - Loss: [2.75281239]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3557/8000 - Min Loss: [2.75 - Loss: [2.7526296]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3558/8000 - Min Loss: [2.75 - Loss: [2.7518468]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3560/8000 - Min Loss: [2.75 - Loss: [2.75131322]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3590/8000 - Min Loss: [2.75 - Loss: [2.75099876]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3606/8000 - Min Loss: [2.75 - Loss: [2.750542]1]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3607/8000 - Min Loss: [2.74 - Loss: [2.74993134]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3608/8000 - Min Loss: [2.74 - Loss: [2.74918492]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3609/8000 - Min Loss: [2.74 - Loss: [2.74904763]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3610/8000 - Min Loss: [2.74 - Loss: [2.74866728]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3611/8000 - Min Loss: [2.74 - Loss: [2.74794943]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3612/8000 - Min Loss: [2.74 - Loss: [2.74769831]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3613/8000 - Min Loss: [2.74 - Loss: [2.74697072]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3630/8000 - Min Loss: [2.74 - Loss: [2.74714045]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3631/8000 - Min Loss: [2.74 - Loss: [2.74650738]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3632/8000 - Min Loss: [2.74 - Loss: [2.74574369]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3633/8000 - Min Loss: [2.74 - Loss: [2.7453696]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3634/8000 - Min Loss: [2.74 - Loss: [2.74463773]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3642/8000 - Min Loss: [2.74 - Loss: [2.74458893]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3643/8000 - Min Loss: [2.74 - Loss: [2.74398048]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3650/8000 - Min Loss: [2.74 - Loss: [2.74367803]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3651/8000 - Min Loss: [2.74 - Loss: [2.74314613]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3664/8000 - Min Loss: [2.74 - Loss: [2.7431059]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3666/8000 - Min Loss: [2.74 - Loss: [2.74251056]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3667/8000 - Min Loss: [2.74 - Loss: [2.74193081]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3670/8000 - Min Loss: [2.74 - Loss: [2.74166497]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3672/8000 - Min Loss: [2.74 - Loss: [2.74152822]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3776/8000 - Min Loss: [2.74 - Loss: [2.74083092]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3777/8000 - Min Loss: [2.74 - Loss: [2.74012576]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3778/8000 - Min Loss: [2.73 - Loss: [2.73955772]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3779/8000 - Min Loss: [2.73 - Loss: [2.73918226]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3781/8000 - Min Loss: [2.73 - Loss: [2.73918549]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3782/8000 - Min Loss: [2.73 - Loss: [2.73853062]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3783/8000 - Min Loss: [2.73 - Loss: [2.73786635]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3784/8000 - Min Loss: [2.73 - Loss: [2.73738306]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3815/8000 - Min Loss: [2.73 - Loss: [2.73698326]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3816/8000 - Min Loss: [2.73 - Loss: [2.73631994]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3817/8000 - Min Loss: [2.73 - Loss: [2.73587864]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3818/8000 - Min Loss: [2.73 - Loss: [2.73519311]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3819/8000 - Min Loss: [2.73 - Loss: [2.73469578]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3820/8000 - Min Loss: [2.73 - Loss: [2.73400114]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3821/8000 - Min Loss: [2.73 - Loss: [2.73349261]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3822/8000 - Min Loss: [2.73 - Loss: [2.73279163]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3823/8000 - Min Loss: [2.73 - Loss: [2.73244817]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3824/8000 - Min Loss: [2.73 - Loss: [2.73184062]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3825/8000 - Min Loss: [2.73 - Loss: [2.73112569]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3827/8000 - Min Loss: [2.73 - Loss: [2.73048537]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3828/8000 - Min Loss: [2.73 - Loss: [2.73009965]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3832/8000 - Min Loss: [2.72 - Loss: [2.72975479]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3842/8000 - Min Loss: [2.72 - Loss: [2.72949549]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3843/8000 - Min Loss: [2.72 - Loss: [2.72884532]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3861/8000 - Min Loss: [2.72 - Loss: [2.72869797]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3876/8000 - Min Loss: [2.72 - Loss: [2.72832613]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3877/8000 - Min Loss: [2.72 - Loss: [2.72763065]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3878/8000 - Min Loss: [2.72 - Loss: [2.72692554]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3880/8000 - Min Loss: [2.72 - Loss: [2.72661154]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3881/8000 - Min Loss: [2.72 - Loss: [2.72606466]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3882/8000 - Min Loss: [2.72 - Loss: [2.72556276]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3883/8000 - Min Loss: [2.72 - Loss: [2.72486251]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3884/8000 - Min Loss: [2.72 - Loss: [2.72417183]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3890/8000 - Min Loss: [2.72 - Loss: [2.72433449]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3895/8000 - Min Loss: [2.72 - Loss: [2.7239603]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3896/8000 - Min Loss: [2.72 - Loss: [2.72333438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha: 0.05 - Example 3897/8000 - Min Loss: [2.72 - Loss: [2.72267364]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3899/8000 - Min Loss: [2.72 - Loss: [2.72233329]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3900/8000 - Min Loss: [2.72 - Loss: [2.72180649]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3901/8000 - Min Loss: [2.72 - Loss: [2.72121077]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3902/8000 - Min Loss: [2.72 - Loss: [2.72109326]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3903/8000 - Min Loss: [2.72 - Loss: [2.72046223]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3904/8000 - Min Loss: [2.72 - Loss: [2.72027293]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3905/8000 - Min Loss: [2.71 - Loss: [2.71975862]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3906/8000 - Min Loss: [2.71 - Loss: [2.71914672]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3909/8000 - Min Loss: [2.71 - Loss: [2.71895308]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3910/8000 - Min Loss: [2.71 - Loss: [2.71829322]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3911/8000 - Min Loss: [2.71 - Loss: [2.71760853]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3922/8000 - Min Loss: [2.71 - Loss: [2.71784827]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3923/8000 - Min Loss: [2.71 - Loss: [2.71716712]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3940/8000 - Min Loss: [2.71 - Loss: [2.71705954]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3941/8000 - Min Loss: [2.71 - Loss: [2.71643437]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3942/8000 - Min Loss: [2.71 - Loss: [2.71574844]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3943/8000 - Min Loss: [2.71 - Loss: [2.71506882]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3944/8000 - Min Loss: [2.71 - Loss: [2.71447902]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3946/8000 - Min Loss: [2.71 - Loss: [2.71481843]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3947/8000 - Min Loss: [2.71 - Loss: [2.7141318]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3948/8000 - Min Loss: [2.71 - Loss: [2.71344981]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3949/8000 - Min Loss: [2.71 - Loss: [2.71286969]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3950/8000 - Min Loss: [2.71 - Loss: [2.71229904]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3956/8000 - Min Loss: [2.71 - Loss: [2.71192189]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3957/8000 - Min Loss: [2.71 - Loss: [2.71153311]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3958/8000 - Min Loss: [2.71 - Loss: [2.71115142]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3959/8000 - Min Loss: [2.71 - Loss: [2.71103935]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3960/8000 - Min Loss: [2.71 - Loss: [2.71087978]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3967/8000 - Min Loss: [2.71 - Loss: [2.7107393]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3968/8000 - Min Loss: [2.71 - Loss: [2.71006947]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3969/8000 - Min Loss: [2.70 - Loss: [2.70938886]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3970/8000 - Min Loss: [2.70 - Loss: [2.70872349]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3971/8000 - Min Loss: [2.70 - Loss: [2.7086145]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3972/8000 - Min Loss: [2.70 - Loss: [2.70812135]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3974/8000 - Min Loss: [2.70 - Loss: [2.70801569]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3975/8000 - Min Loss: [2.70 - Loss: [2.70734401]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3976/8000 - Min Loss: [2.70 - Loss: [2.70685108]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3977/8000 - Min Loss: [2.70 - Loss: [2.70684114]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3978/8000 - Min Loss: [2.70 - Loss: [2.70617298]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3993/8000 - Min Loss: [2.70 - Loss: [2.70613453]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3994/8000 - Min Loss: [2.70 - Loss: [2.70557301]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3995/8000 - Min Loss: [2.70 - Loss: [2.70490608]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3996/8000 - Min Loss: [2.70 - Loss: [2.70423625]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3997/8000 - Min Loss: [2.70 - Loss: [2.70389148]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3998/8000 - Min Loss: [2.70 - Loss: [2.70324842]\n",
      "Iter: 0 - Alpha: 0.05 - Example 3999/8000 - Min Loss: [2.70 - Loss: [2.70271482]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4000/8000 - Min Loss: [2.70 - Loss: [2.70204508]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4006/8000 - Min Loss: [2.70 - Loss: [2.70186224]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4007/8000 - Min Loss: [2.70 - Loss: [2.70145787]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4009/8000 - Min Loss: [2.70 - Loss: [2.70152783]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4032/8000 - Min Loss: [2.70 - Loss: [2.70107328]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4033/8000 - Min Loss: [2.70 - Loss: [2.70084527]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4034/8000 - Min Loss: [2.70 - Loss: [2.70027544]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4035/8000 - Min Loss: [2.69 - Loss: [2.69987502]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4036/8000 - Min Loss: [2.69 - Loss: [2.6992189]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4037/8000 - Min Loss: [2.69 - Loss: [2.69873288]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4038/8000 - Min Loss: [2.69 - Loss: [2.6983587]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4042/8000 - Min Loss: [2.69 - Loss: [2.69843893]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4043/8000 - Min Loss: [2.69 - Loss: [2.69781691]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4044/8000 - Min Loss: [2.69 - Loss: [2.69715664]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4045/8000 - Min Loss: [2.69 - Loss: [2.69706953]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4118/8000 - Min Loss: [2.69 - Loss: [2.69685497]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4119/8000 - Min Loss: [2.69 - Loss: [2.69628661]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4120/8000 - Min Loss: [2.69 - Loss: [2.69576359]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4121/8000 - Min Loss: [2.69 - Loss: [2.69512144]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4122/8000 - Min Loss: [2.69 - Loss: [2.69462147]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4133/8000 - Min Loss: [2.69 - Loss: [2.69460915]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4134/8000 - Min Loss: [2.69 - Loss: [2.69416966]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4137/8000 - Min Loss: [2.69 - Loss: [2.69358001]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4138/8000 - Min Loss: [2.69 - Loss: [2.69294772]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4139/8000 - Min Loss: [2.69 - Loss: [2.69234925]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4140/8000 - Min Loss: [2.69 - Loss: [2.69176279]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4141/8000 - Min Loss: [2.69 - Loss: [2.69155703]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4142/8000 - Min Loss: [2.69 - Loss: [2.69096266]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4143/8000 - Min Loss: [2.69 - Loss: [2.69089113]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4144/8000 - Min Loss: [2.69 - Loss: [2.69068594]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4145/8000 - Min Loss: [2.69 - Loss: [2.69057498]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4146/8000 - Min Loss: [2.68 - Loss: [2.68994422]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4148/8000 - Min Loss: [2.68 - Loss: [2.68996694]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4167/8000 - Min Loss: [2.68 - Loss: [2.6900127]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4168/8000 - Min Loss: [2.68 - Loss: [2.68955253]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4172/8000 - Min Loss: [2.68 - Loss: [2.68925423]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4180/8000 - Min Loss: [2.68 - Loss: [2.68931412]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4181/8000 - Min Loss: [2.68 - Loss: [2.68879362]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4182/8000 - Min Loss: [2.68 - Loss: [2.68816485]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4183/8000 - Min Loss: [2.68 - Loss: [2.68776932]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4216/8000 - Min Loss: [2.68 - Loss: [2.68758537]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4217/8000 - Min Loss: [2.68 - Loss: [2.68722249]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4218/8000 - Min Loss: [2.68 - Loss: [2.6866684]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4219/8000 - Min Loss: [2.68 - Loss: [2.68615284]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4220/8000 - Min Loss: [2.68 - Loss: [2.68566427]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4221/8000 - Min Loss: [2.68 - Loss: [2.68505003]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4223/8000 - Min Loss: [2.68 - Loss: [2.68494745]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4242/8000 - Min Loss: [2.68 - Loss: [2.68459279]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4243/8000 - Min Loss: [2.68 - Loss: [2.68444245]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4244/8000 - Min Loss: [2.68 - Loss: [2.68382006]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4245/8000 - Min Loss: [2.68 - Loss: [2.68321793]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4246/8000 - Min Loss: [2.68 - Loss: [2.68271656]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4247/8000 - Min Loss: [2.68 - Loss: [2.68214591]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4248/8000 - Min Loss: [2.68 - Loss: [2.68158879]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4249/8000 - Min Loss: [2.68 - Loss: [2.68098793]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4250/8000 - Min Loss: [2.68 - Loss: [2.68037754]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4251/8000 - Min Loss: [2.67 - Loss: [2.67975621]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4252/8000 - Min Loss: [2.67 - Loss: [2.67922779]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4253/8000 - Min Loss: [2.67 - Loss: [2.67907061]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4254/8000 - Min Loss: [2.67 - Loss: [2.67850196]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4255/8000 - Min Loss: [2.67 - Loss: [2.67822761]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4256/8000 - Min Loss: [2.67 - Loss: [2.67772874]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4257/8000 - Min Loss: [2.67 - Loss: [2.67710929]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4258/8000 - Min Loss: [2.67 - Loss: [2.67650662]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4260/8000 - Min Loss: [2.67 - Loss: [2.67628042]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4262/8000 - Min Loss: [2.67 - Loss: [2.67570336]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4263/8000 - Min Loss: [2.67 - Loss: [2.6751434]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4264/8000 - Min Loss: [2.67 - Loss: [2.67458013]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4265/8000 - Min Loss: [2.67 - Loss: [2.6740815]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4275/8000 - Min Loss: [2.67 - Loss: [2.6738262]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4276/8000 - Min Loss: [2.67 - Loss: [2.6734167]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4277/8000 - Min Loss: [2.67 - Loss: [2.67300589]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4278/8000 - Min Loss: [2.67 - Loss: [2.67240062]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4280/8000 - Min Loss: [2.67 - Loss: [2.67233985]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4281/8000 - Min Loss: [2.67 - Loss: [2.67194935]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4282/8000 - Min Loss: [2.67 - Loss: [2.67160015]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4283/8000 - Min Loss: [2.67 - Loss: [2.6710966]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4284/8000 - Min Loss: [2.67 - Loss: [2.67071655]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4285/8000 - Min Loss: [2.67 - Loss: [2.67042473]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4286/8000 - Min Loss: [2.67 - Loss: [2.67014443]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4287/8000 - Min Loss: [2.66 - Loss: [2.66964048]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4288/8000 - Min Loss: [2.66 - Loss: [2.66931026]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4302/8000 - Min Loss: [2.66 - Loss: [2.66930133]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4303/8000 - Min Loss: [2.66 - Loss: [2.66873608]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4304/8000 - Min Loss: [2.66 - Loss: [2.66822481]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4305/8000 - Min Loss: [2.66 - Loss: [2.66767165]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4306/8000 - Min Loss: [2.66 - Loss: [2.6672275]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4307/8000 - Min Loss: [2.66 - Loss: [2.66697132]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4373/8000 - Min Loss: [2.66 - Loss: [2.66666079]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4374/8000 - Min Loss: [2.66 - Loss: [2.66633855]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4375/8000 - Min Loss: [2.66 - Loss: [2.66576312]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4376/8000 - Min Loss: [2.66 - Loss: [2.66532473]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4377/8000 - Min Loss: [2.66 - Loss: [2.66525489]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4378/8000 - Min Loss: [2.66 - Loss: [2.66489463]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4380/8000 - Min Loss: [2.66 - Loss: [2.6649243]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4381/8000 - Min Loss: [2.66 - Loss: [2.66438524]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4382/8000 - Min Loss: [2.66 - Loss: [2.6637895]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4402/8000 - Min Loss: [2.66 - Loss: [2.66373122]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4403/8000 - Min Loss: [2.66 - Loss: [2.66322954]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4404/8000 - Min Loss: [2.66 - Loss: [2.66275178]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4405/8000 - Min Loss: [2.66 - Loss: [2.6622269]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4406/8000 - Min Loss: [2.66 - Loss: [2.66169277]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4407/8000 - Min Loss: [2.66 - Loss: [2.66158516]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4408/8000 - Min Loss: [2.66 - Loss: [2.66145209]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4409/8000 - Min Loss: [2.66 - Loss: [2.66108549]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4410/8000 - Min Loss: [2.66 - Loss: [2.66058451]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4411/8000 - Min Loss: [2.66 - Loss: [2.66008403]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4412/8000 - Min Loss: [2.65 - Loss: [2.6598203]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4413/8000 - Min Loss: [2.65 - Loss: [2.65926958]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4414/8000 - Min Loss: [2.65 - Loss: [2.65870177]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4416/8000 - Min Loss: [2.65 - Loss: [2.65852538]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4417/8000 - Min Loss: [2.65 - Loss: [2.65808186]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4418/8000 - Min Loss: [2.65 - Loss: [2.65796168]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4526/8000 - Min Loss: [2.65 - Loss: [2.65788305]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4548/8000 - Min Loss: [2.65 - Loss: [2.65754275]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4549/8000 - Min Loss: [2.65 - Loss: [2.65699651]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4550/8000 - Min Loss: [2.65 - Loss: [2.65658916]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4552/8000 - Min Loss: [2.65 - Loss: [2.65667857]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4553/8000 - Min Loss: [2.65 - Loss: [2.6563245]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4554/8000 - Min Loss: [2.65 - Loss: [2.65575677]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4557/8000 - Min Loss: [2.65 - Loss: [2.65556587]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4558/8000 - Min Loss: [2.65 - Loss: [2.65499732]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4559/8000 - Min Loss: [2.65 - Loss: [2.65443086]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4561/8000 - Min Loss: [2.65 - Loss: [2.65416563]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4562/8000 - Min Loss: [2.65 - Loss: [2.65392003]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4563/8000 - Min Loss: [2.65 - Loss: [2.65335394]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4564/8000 - Min Loss: [2.65 - Loss: [2.65292417]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4565/8000 - Min Loss: [2.65 - Loss: [2.65241951]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4566/8000 - Min Loss: [2.65 - Loss: [2.65235791]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4909/8000 - Min Loss: [2.65 - Loss: [2.65226387]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4910/8000 - Min Loss: [2.65 - Loss: [2.65174424]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4911/8000 - Min Loss: [2.65 - Loss: [2.65129963]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4912/8000 - Min Loss: [2.65 - Loss: [2.65105051]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4913/8000 - Min Loss: [2.65 - Loss: [2.65083071]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4914/8000 - Min Loss: [2.65 - Loss: [2.65067997]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4915/8000 - Min Loss: [2.65 - Loss: [2.6501602]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4925/8000 - Min Loss: [2.64 - Loss: [2.6497831]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4926/8000 - Min Loss: [2.64 - Loss: [2.64945982]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4927/8000 - Min Loss: [2.64 - Loss: [2.64926544]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4928/8000 - Min Loss: [2.64 - Loss: [2.64894102]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4929/8000 - Min Loss: [2.64 - Loss: [2.64883326]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4930/8000 - Min Loss: [2.64 - Loss: [2.64835134]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4931/8000 - Min Loss: [2.64 - Loss: [2.64785162]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4932/8000 - Min Loss: [2.64 - Loss: [2.64760526]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4937/8000 - Min Loss: [2.64 - Loss: [2.64738356]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4938/8000 - Min Loss: [2.64 - Loss: [2.64701238]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4939/8000 - Min Loss: [2.64 - Loss: [2.6468221]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4940/8000 - Min Loss: [2.64 - Loss: [2.64630547]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4941/8000 - Min Loss: [2.64 - Loss: [2.64585427]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4942/8000 - Min Loss: [2.64 - Loss: [2.64557198]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4943/8000 - Min Loss: [2.64 - Loss: [2.64537183]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4945/8000 - Min Loss: [2.64 - Loss: [2.64486762]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4946/8000 - Min Loss: [2.64 - Loss: [2.64437081]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4947/8000 - Min Loss: [2.64 - Loss: [2.64389359]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4948/8000 - Min Loss: [2.64 - Loss: [2.64337434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha: 0.05 - Example 4950/8000 - Min Loss: [2.64 - Loss: [2.6432193]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 4951/8000 - Min Loss: [2.64 - Loss: [2.64280772]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5003/8000 - Min Loss: [2.64 - Loss: [2.64277898]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5004/8000 - Min Loss: [2.64 - Loss: [2.64250768]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5005/8000 - Min Loss: [2.64 - Loss: [2.64206744]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5006/8000 - Min Loss: [2.64 - Loss: [2.64160358]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5007/8000 - Min Loss: [2.64 - Loss: [2.64126039]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5008/8000 - Min Loss: [2.64 - Loss: [2.64120657]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5009/8000 - Min Loss: [2.64 - Loss: [2.6408765]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5016/8000 - Min Loss: [2.64 - Loss: [2.64086321]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5017/8000 - Min Loss: [2.64 - Loss: [2.6403531]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5019/8000 - Min Loss: [2.64 - Loss: [2.64041153]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5020/8000 - Min Loss: [2.63 - Loss: [2.63994237]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5021/8000 - Min Loss: [2.63 - Loss: [2.63961169]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5022/8000 - Min Loss: [2.63 - Loss: [2.63925884]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5023/8000 - Min Loss: [2.63 - Loss: [2.63875324]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5024/8000 - Min Loss: [2.63 - Loss: [2.63840194]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5025/8000 - Min Loss: [2.63 - Loss: [2.63832163]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5026/8000 - Min Loss: [2.63 - Loss: [2.63795478]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5027/8000 - Min Loss: [2.63 - Loss: [2.63746792]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5029/8000 - Min Loss: [2.63 - Loss: [2.63709011]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5031/8000 - Min Loss: [2.63 - Loss: [2.63695778]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5036/8000 - Min Loss: [2.63 - Loss: [2.63666691]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5037/8000 - Min Loss: [2.63 - Loss: [2.63653557]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5038/8000 - Min Loss: [2.63 - Loss: [2.63605683]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5040/8000 - Min Loss: [2.63 - Loss: [2.63580802]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5041/8000 - Min Loss: [2.63 - Loss: [2.63552825]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5042/8000 - Min Loss: [2.63 - Loss: [2.63510207]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5043/8000 - Min Loss: [2.63 - Loss: [2.63471295]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5044/8000 - Min Loss: [2.63 - Loss: [2.63426652]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5084/8000 - Min Loss: [2.63 - Loss: [2.63398646]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5085/8000 - Min Loss: [2.63 - Loss: [2.63348677]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5086/8000 - Min Loss: [2.63 - Loss: [2.63323647]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5087/8000 - Min Loss: [2.63 - Loss: [2.63286093]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5088/8000 - Min Loss: [2.63 - Loss: [2.63271553]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5089/8000 - Min Loss: [2.63 - Loss: [2.63222554]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5090/8000 - Min Loss: [2.63 - Loss: [2.63181849]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5091/8000 - Min Loss: [2.63 - Loss: [2.63143939]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5092/8000 - Min Loss: [2.63 - Loss: [2.63098004]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5098/8000 - Min Loss: [2.63 - Loss: [2.63111079]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5099/8000 - Min Loss: [2.63 - Loss: [2.63064879]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5101/8000 - Min Loss: [2.63 - Loss: [2.63055358]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5102/8000 - Min Loss: [2.63 - Loss: [2.63005496]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5112/8000 - Min Loss: [2.62 - Loss: [2.62993582]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5113/8000 - Min Loss: [2.62 - Loss: [2.6295727]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5114/8000 - Min Loss: [2.62 - Loss: [2.62917546]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5118/8000 - Min Loss: [2.62 - Loss: [2.6290484]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5119/8000 - Min Loss: [2.62 - Loss: [2.62863295]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5120/8000 - Min Loss: [2.62 - Loss: [2.62813686]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5133/8000 - Min Loss: [2.62 - Loss: [2.62776039]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5145/8000 - Min Loss: [2.62 - Loss: [2.62747828]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5146/8000 - Min Loss: [2.62 - Loss: [2.62717454]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5147/8000 - Min Loss: [2.62 - Loss: [2.62668383]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5159/8000 - Min Loss: [2.62 - Loss: [2.62671088]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5160/8000 - Min Loss: [2.62 - Loss: [2.62625604]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5161/8000 - Min Loss: [2.62 - Loss: [2.62580945]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5434/8000 - Min Loss: [2.62 - Loss: [2.62551718]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5435/8000 - Min Loss: [2.62 - Loss: [2.62505183]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5436/8000 - Min Loss: [2.62 - Loss: [2.62466095]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5437/8000 - Min Loss: [2.62 - Loss: [2.62426616]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5438/8000 - Min Loss: [2.62 - Loss: [2.62423729]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5822/8000 - Min Loss: [2.62 - Loss: [2.62412744]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5823/8000 - Min Loss: [2.62 - Loss: [2.62369309]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5824/8000 - Min Loss: [2.62 - Loss: [2.62332773]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5915/8000 - Min Loss: [2.62 - Loss: [2.62350528]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5953/8000 - Min Loss: [2.62 - Loss: [2.62323945]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5961/8000 - Min Loss: [2.62 - Loss: [2.62308619]\n",
      "Iter: 0 - Alpha: 0.05 - Example 5962/8000 - Min Loss: [2.62 - Loss: [2.62267997]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6006/8000 - Min Loss: [2.62 - Loss: [2.62232101]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6033/8000 - Min Loss: [2.62 - Loss: [2.62230427]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6035/8000 - Min Loss: [2.62 - Loss: [2.62190915]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6036/8000 - Min Loss: [2.62 - Loss: [2.62149063]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6037/8000 - Min Loss: [2.62 - Loss: [2.62107316]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6038/8000 - Min Loss: [2.62 - Loss: [2.62066292]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6045/8000 - Min Loss: [2.62 - Loss: [2.62087137]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6047/8000 - Min Loss: [2.62 - Loss: [2.62050799]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6303/8000 - Min Loss: [2.62 - Loss: [2.62052427]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6304/8000 - Min Loss: [2.62 - Loss: [2.62016797]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6305/8000 - Min Loss: [2.61 - Loss: [2.61980036]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6306/8000 - Min Loss: [2.61 - Loss: [2.61940044]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6307/8000 - Min Loss: [2.61 - Loss: [2.61933713]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6315/8000 - Min Loss: [2.61 - Loss: [2.61908314]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6316/8000 - Min Loss: [2.61 - Loss: [2.61880165]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6317/8000 - Min Loss: [2.61 - Loss: [2.6184035]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6328/8000 - Min Loss: [2.61 - Loss: [2.61825817]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6329/8000 - Min Loss: [2.61 - Loss: [2.6178773]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6330/8000 - Min Loss: [2.61 - Loss: [2.61749249]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6331/8000 - Min Loss: [2.61 - Loss: [2.61727691]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6332/8000 - Min Loss: [2.61 - Loss: [2.61695996]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6333/8000 - Min Loss: [2.61 - Loss: [2.61683114]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6334/8000 - Min Loss: [2.61 - Loss: [2.61661163]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6335/8000 - Min Loss: [2.61 - Loss: [2.61633917]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6359/8000 - Min Loss: [2.61 - Loss: [2.61599458]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6360/8000 - Min Loss: [2.61 - Loss: [2.61578461]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6361/8000 - Min Loss: [2.61 - Loss: [2.6156865]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6364/8000 - Min Loss: [2.61 - Loss: [2.6156652]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6385/8000 - Min Loss: [2.61 - Loss: [2.61550891]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6399/8000 - Min Loss: [2.61 - Loss: [2.61523003]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6400/8000 - Min Loss: [2.61 - Loss: [2.6148595]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6401/8000 - Min Loss: [2.61 - Loss: [2.61459112]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6402/8000 - Min Loss: [2.61 - Loss: [2.61422491]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6430/8000 - Min Loss: [2.61 - Loss: [2.61420117]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6431/8000 - Min Loss: [2.61 - Loss: [2.61382111]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6432/8000 - Min Loss: [2.61 - Loss: [2.61343074]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6457/8000 - Min Loss: [2.61 - Loss: [2.61327057]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6467/8000 - Min Loss: [2.61 - Loss: [2.61309188]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6468/8000 - Min Loss: [2.61 - Loss: [2.61270428]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6469/8000 - Min Loss: [2.61 - Loss: [2.61241713]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6470/8000 - Min Loss: [2.61 - Loss: [2.61204147]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6471/8000 - Min Loss: [2.61 - Loss: [2.61172756]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6472/8000 - Min Loss: [2.61 - Loss: [2.61145787]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6473/8000 - Min Loss: [2.61 - Loss: [2.6112936]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6474/8000 - Min Loss: [2.61 - Loss: [2.61103473]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6475/8000 - Min Loss: [2.61 - Loss: [2.61092225]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6476/8000 - Min Loss: [2.61 - Loss: [2.61056284]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6477/8000 - Min Loss: [2.61 - Loss: [2.61030041]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6478/8000 - Min Loss: [2.61 - Loss: [2.61002966]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6485/8000 - Min Loss: [2.60 - Loss: [2.60978115]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6486/8000 - Min Loss: [2.60 - Loss: [2.60940427]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6487/8000 - Min Loss: [2.60 - Loss: [2.60928681]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6488/8000 - Min Loss: [2.60 - Loss: [2.60892681]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6544/8000 - Min Loss: [2.60 - Loss: [2.60863408]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6545/8000 - Min Loss: [2.60 - Loss: [2.60853321]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6546/8000 - Min Loss: [2.60 - Loss: [2.60847284]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6547/8000 - Min Loss: [2.60 - Loss: [2.6084464]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6548/8000 - Min Loss: [2.60 - Loss: [2.60827403]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6549/8000 - Min Loss: [2.60 - Loss: [2.60804157]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6551/8000 - Min Loss: [2.60 - Loss: [2.6078768]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6552/8000 - Min Loss: [2.60 - Loss: [2.60750401]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6558/8000 - Min Loss: [2.60 - Loss: [2.60737801]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6756/8000 - Min Loss: [2.60 - Loss: [2.60711314]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6757/8000 - Min Loss: [2.60 - Loss: [2.60674384]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6758/8000 - Min Loss: [2.60 - Loss: [2.60671137]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6759/8000 - Min Loss: [2.60 - Loss: [2.60665377]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6788/8000 - Min Loss: [2.60 - Loss: [2.60666809]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6789/8000 - Min Loss: [2.60 - Loss: [2.60630047]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6790/8000 - Min Loss: [2.60 - Loss: [2.60603628]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6791/8000 - Min Loss: [2.60 - Loss: [2.60593033]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6792/8000 - Min Loss: [2.60 - Loss: [2.60558045]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6793/8000 - Min Loss: [2.60 - Loss: [2.60525729]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6794/8000 - Min Loss: [2.60 - Loss: [2.60491253]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6795/8000 - Min Loss: [2.60 - Loss: [2.60458176]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6796/8000 - Min Loss: [2.60 - Loss: [2.60430197]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6797/8000 - Min Loss: [2.60 - Loss: [2.60422951]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6799/8000 - Min Loss: [2.60 - Loss: [2.60417104]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6800/8000 - Min Loss: [2.60 - Loss: [2.60391841]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6802/8000 - Min Loss: [2.60 - Loss: [2.60357715]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6803/8000 - Min Loss: [2.60 - Loss: [2.60343053]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6804/8000 - Min Loss: [2.60 - Loss: [2.60306477]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6805/8000 - Min Loss: [2.60 - Loss: [2.60271226]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6806/8000 - Min Loss: [2.60 - Loss: [2.60236118]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6807/8000 - Min Loss: [2.60 - Loss: [2.60200089]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6808/8000 - Min Loss: [2.60 - Loss: [2.60174448]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6809/8000 - Min Loss: [2.60 - Loss: [2.60150826]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6810/8000 - Min Loss: [2.60 - Loss: [2.60125931]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6812/8000 - Min Loss: [2.60 - Loss: [2.60112068]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6827/8000 - Min Loss: [2.60 - Loss: [2.60118838]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6828/8000 - Min Loss: [2.60 - Loss: [2.60086474]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6829/8000 - Min Loss: [2.60 - Loss: [2.60074989]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6830/8000 - Min Loss: [2.60 - Loss: [2.60070654]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6831/8000 - Min Loss: [2.60 - Loss: [2.60036095]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6832/8000 - Min Loss: [2.60 - Loss: [2.60002124]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6836/8000 - Min Loss: [2.59 - Loss: [2.5999089]]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6837/8000 - Min Loss: [2.59 - Loss: [2.59964381]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6838/8000 - Min Loss: [2.59 - Loss: [2.59941079]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6839/8000 - Min Loss: [2.59 - Loss: [2.59910952]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6840/8000 - Min Loss: [2.59 - Loss: [2.59882415]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6844/8000 - Min Loss: [2.59 - Loss: [2.59878363]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6845/8000 - Min Loss: [2.59 - Loss: [2.59845593]\n",
      "Iter: 0 - Alpha: 0.05 - Example 6846/8000 - Min Loss: [2.59 - Loss: [2.59819716]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7599/8000 - Min Loss: [2.59 - Loss: [2.59818381]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7629/8000 - Min Loss: [2.59 - Loss: [2.59815036]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7630/8000 - Min Loss: [2.59 - Loss: [2.59785002]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7631/8000 - Min Loss: [2.59 - Loss: [2.59754946]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7632/8000 - Min Loss: [2.59 - Loss: [2.59736978]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7633/8000 - Min Loss: [2.59 - Loss: [2.59706904]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7634/8000 - Min Loss: [2.59 - Loss: [2.59676305]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7635/8000 - Min Loss: [2.59 - Loss: [2.59644749]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7636/8000 - Min Loss: [2.59 - Loss: [2.59635429]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7637/8000 - Min Loss: [2.59 - Loss: [2.59604597]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7645/8000 - Min Loss: [2.59 - Loss: [2.59607592]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7648/8000 - Min Loss: [2.59 - Loss: [2.59588988]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7649/8000 - Min Loss: [2.59 - Loss: [2.59556692]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7650/8000 - Min Loss: [2.59 - Loss: [2.59534633]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7655/8000 - Min Loss: [2.59 - Loss: [2.59518858]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7656/8000 - Min Loss: [2.59 - Loss: [2.59496733]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7657/8000 - Min Loss: [2.59 - Loss: [2.59467017]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7658/8000 - Min Loss: [2.59 - Loss: [2.59434963]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7662/8000 - Min Loss: [2.59 - Loss: [2.59405208]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7663/8000 - Min Loss: [2.59 - Loss: [2.59373129]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7664/8000 - Min Loss: [2.59 - Loss: [2.59341357]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7665/8000 - Min Loss: [2.59 - Loss: [2.59319059]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7667/8000 - Min Loss: [2.59 - Loss: [2.59307048]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7669/8000 - Min Loss: [2.59 - Loss: [2.59285634]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7670/8000 - Min Loss: [2.59 - Loss: [2.59260098]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7671/8000 - Min Loss: [2.59 - Loss: [2.59258135]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7672/8000 - Min Loss: [2.59 - Loss: [2.59230227]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7676/8000 - Min Loss: [2.59 - Loss: [2.59201554]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7677/8000 - Min Loss: [2.59 - Loss: [2.59171595]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7678/8000 - Min Loss: [2.59 - Loss: [2.59140566]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7681/8000 - Min Loss: [2.59 - Loss: [2.59126724]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7682/8000 - Min Loss: [2.59 - Loss: [2.59094738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha: 0.05 - Example 7696/8000 - Min Loss: [2.59 - Loss: [2.59080944]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7706/8000 - Min Loss: [2.59 - Loss: [2.59079672]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7707/8000 - Min Loss: [2.59 - Loss: [2.59047673]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7745/8000 - Min Loss: [2.59 - Loss: [2.59051556]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7746/8000 - Min Loss: [2.59 - Loss: [2.59026318]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7747/8000 - Min Loss: [2.59 - Loss: [2.59008117]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7748/8000 - Min Loss: [2.58 - Loss: [2.589794]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7749/8000 - Min Loss: [2.58 - Loss: [2.58947661]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7752/8000 - Min Loss: [2.58 - Loss: [2.58924038]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7753/8000 - Min Loss: [2.58 - Loss: [2.58900352]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7754/8000 - Min Loss: [2.58 - Loss: [2.58869659]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7755/8000 - Min Loss: [2.58 - Loss: [2.58844102]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7756/8000 - Min Loss: [2.58 - Loss: [2.58812495]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7757/8000 - Min Loss: [2.58 - Loss: [2.5878347]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7790/8000 - Min Loss: [2.58 - Loss: [2.58763724]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7805/8000 - Min Loss: [2.58 - Loss: [2.58759941]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7806/8000 - Min Loss: [2.58 - Loss: [2.58732353]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7807/8000 - Min Loss: [2.58 - Loss: [2.58701376]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7808/8000 - Min Loss: [2.58 - Loss: [2.58676046]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7809/8000 - Min Loss: [2.58 - Loss: [2.58645638]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7810/8000 - Min Loss: [2.58 - Loss: [2.58617055]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7811/8000 - Min Loss: [2.58 - Loss: [2.58606197]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7812/8000 - Min Loss: [2.58 - Loss: [2.58578767]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7813/8000 - Min Loss: [2.58 - Loss: [2.58577589]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7815/8000 - Min Loss: [2.58 - Loss: [2.58595775]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7862/8000 - Min Loss: [2.58 - Loss: [2.58590819]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7863/8000 - Min Loss: [2.58 - Loss: [2.58568419]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7864/8000 - Min Loss: [2.58 - Loss: [2.58539421]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7865/8000 - Min Loss: [2.58 - Loss: [2.58509292]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7866/8000 - Min Loss: [2.58 - Loss: [2.58479775]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7867/8000 - Min Loss: [2.58 - Loss: [2.58448877]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7868/8000 - Min Loss: [2.58 - Loss: [2.58417701]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7869/8000 - Min Loss: [2.58 - Loss: [2.5839234]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7870/8000 - Min Loss: [2.58 - Loss: [2.5836547]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7872/8000 - Min Loss: [2.58 - Loss: [2.58348299]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7873/8000 - Min Loss: [2.58 - Loss: [2.58318185]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7876/8000 - Min Loss: [2.58 - Loss: [2.58293688]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7878/8000 - Min Loss: [2.58 - Loss: [2.58269981]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7879/8000 - Min Loss: [2.58 - Loss: [2.58239455]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7880/8000 - Min Loss: [2.58 - Loss: [2.58230307]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7881/8000 - Min Loss: [2.58 - Loss: [2.58221033]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7882/8000 - Min Loss: [2.58 - Loss: [2.58190622]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7886/8000 - Min Loss: [2.58 - Loss: [2.58194665]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7887/8000 - Min Loss: [2.58 - Loss: [2.58163704]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7888/8000 - Min Loss: [2.58 - Loss: [2.58144758]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7910/8000 - Min Loss: [2.58 - Loss: [2.58146582]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7911/8000 - Min Loss: [2.58 - Loss: [2.58115821]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7913/8000 - Min Loss: [2.58 - Loss: [2.58132683]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7914/8000 - Min Loss: [2.58 - Loss: [2.58101821]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7915/8000 - Min Loss: [2.58 - Loss: [2.58071891]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7916/8000 - Min Loss: [2.58 - Loss: [2.58040992]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7917/8000 - Min Loss: [2.58 - Loss: [2.58035225]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7918/8000 - Min Loss: [2.58 - Loss: [2.58020301]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7921/8000 - Min Loss: [2.57 - Loss: [2.58001354]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7922/8000 - Min Loss: [2.57 - Loss: [2.57984374]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7923/8000 - Min Loss: [2.57 - Loss: [2.57980033]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7924/8000 - Min Loss: [2.57 - Loss: [2.57955892]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7925/8000 - Min Loss: [2.57 - Loss: [2.5793238]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7929/8000 - Min Loss: [2.57 - Loss: [2.57924435]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7930/8000 - Min Loss: [2.57 - Loss: [2.57894151]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7932/8000 - Min Loss: [2.57 - Loss: [2.57896969]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7933/8000 - Min Loss: [2.57 - Loss: [2.57869407]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7935/8000 - Min Loss: [2.57 - Loss: [2.57843933]\n",
      "Iter: 0 - Alpha: 0.05 - Example 7942/8000 - Min Loss: [2.57 - Loss: [2.57819202]\n",
      "Iter: 1 - Alpha: 0.049 - Example 3/8000 - Min Loss: [2.57 - Loss: [2.81113324]8]\n",
      "Iter: 1 - Alpha: 0.049 - Example 4/8000 - Min Loss: [2.17 - Loss: [2.17112373]\n",
      "Iter: 1 - Alpha: 0.049 - Example 5/8000 - Min Loss: [1.86 - Loss: [1.865339]\n",
      "Iter: 2 - Alpha: 0.049 - Example 10/8000 - Min Loss: [1.71 - Loss: [1.80783324]6]\n",
      "Iter: 2 - Alpha: 0.049 - Example 18/8000 - Min Loss: [1.71 - Loss: [1.76616506]\n",
      "Iter: 2 - Alpha: 0.049 - Example 19/8000 - Min Loss: [1.71 - Loss: [1.71449049]\n",
      "Iter: 2 - Alpha: 0.049 - Example 20/8000 - Min Loss: [1.67 - Loss: [1.67375118]\n",
      "Iter: 3 - Alpha: 0.048 - Example 5/8000 - Min Loss: [1.66 - Loss: [1.74199781]78]\n",
      "Iter: 4 - Alpha: 0.048 - Example 5/8000 - Min Loss: [1.62 - Loss: [1.69132268]11]\n",
      "Iter: 4 - Alpha: 0.048 - Example 1408/8000 - Min Loss: [1.57 - Loss: [1.90789538]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m target \u001b[38;5;241m=\u001b[39m Tensor(y_train[task_i], autograd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mforward(output, target)\n\u001b[1;32m---> 19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train[task_i])\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:174\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    172\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    173\u001b[0m     copies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[dim]\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mexpand(dim, copies), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op:\n\u001b[0;32m    177\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:132\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    135\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:127\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__neg__\u001b[39m(), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__neg__\u001b[39m(), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:120\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads_accounted_for_all_children() \u001b[38;5;129;01mor\u001b[39;00m grad_origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:139\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    137\u001b[0m     new1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m@\u001b[39m weights\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m    138\u001b[0m     new2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mtranspose() \u001b[38;5;241m@\u001b[39m activation)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m--> 139\u001b[0m     activation\u001b[38;5;241m.\u001b[39mbackward(new1, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    140\u001b[0m     weights\u001b[38;5;241m.\u001b[39mbackward(new2, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:132\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    135\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:153\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# tanh'(x) = 1 - tanh(x) ** 2\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m (ones \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# ReLU'(x) = 1 if x > 0, 0 if x <= 0\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:121\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__neg__\u001b[39m(), \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:132\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    135\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:153\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# tanh'(x) = 1 - tanh(x) ** 2\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m (ones \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# ReLU'(x) = 1 if x > 0, 0 if x <= 0\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:121\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__neg__\u001b[39m(), \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:120\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads_accounted_for_all_children() \u001b[38;5;129;01mor\u001b[39;00m grad_origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:139\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    137\u001b[0m     new1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m@\u001b[39m weights\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m    138\u001b[0m     new2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mtranspose() \u001b[38;5;241m@\u001b[39m activation)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m--> 139\u001b[0m     activation\u001b[38;5;241m.\u001b[39mbackward(new1, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    140\u001b[0m     weights\u001b[38;5;241m.\u001b[39mbackward(new2, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:132\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    135\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:153\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# tanh'(x) = 1 - tanh(x) ** 2\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m (ones \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# ReLU'(x) = 1 if x > 0, 0 if x <= 0\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:121\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__neg__\u001b[39m(), \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:132\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    135\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:153\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# tanh'(x) = 1 - tanh(x) ** 2\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m (ones \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# ReLU'(x) = 1 if x > 0, 0 if x <= 0\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     ones \u001b[38;5;241m=\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata))\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:120\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads_accounted_for_all_children() \u001b[38;5;129;01mor\u001b[39;00m grad_origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:120\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads_accounted_for_all_children() \u001b[38;5;129;01mor\u001b[39;00m grad_origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:140\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    138\u001b[0m     new2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mtranspose() \u001b[38;5;241m@\u001b[39m activation)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m    139\u001b[0m     activation\u001b[38;5;241m.\u001b[39mbackward(new1, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m     weights\u001b[38;5;241m.\u001b[39mbackward(new2, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mtranspose(), \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:115\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad, grad_origin)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m grad\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grad\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Actual backpropagation\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrads_accounted_for_all_children() \u001b[38;5;129;01mor\u001b[39;00m grad_origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Documents\\A Levels\\Computer Science\\NEA\\smart-planner\\prototype\\autograd.py:180\u001b[0m, in \u001b[0;36mTensor.__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    177\u001b[0m                 dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_op\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    178\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39msum(dim), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    Adds two tensors together.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m        The tensor produced when adding the 'other' tensor to this tensor.\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautograd \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mautograd:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_loss = 1000\n",
    "iterations = 10\n",
    "for iteration in range(iterations):\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    \n",
    "    for task_i in range(len(X_train)):\n",
    "        hidden = model.init_hidden(batch_size=1)\n",
    "        \n",
    "        for t in range(len(X_train[task_i])):\n",
    "            input = Tensor([X_train[task_i][t]], autograd=True)\n",
    "            rnn_input = embedding.forward(input=input)\n",
    "            hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        \n",
    "        output = output_layer.forward(hidden[0])\n",
    "        target = Tensor(y_train[task_i], autograd=True)\n",
    "        loss = criterion.forward(output, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        total_loss += loss.data / len(X_train[task_i])\n",
    "        epoch_loss = np.exp(total_loss / (task_i + 1))\n",
    "        \n",
    "        if epoch_loss < min_loss:\n",
    "            min_loss = epoch_loss\n",
    "            print()\n",
    "        \n",
    "        print(f\"Iter: {iteration} - Alpha: {str(optimiser.alpha)[:5]} - Example {task_i + 1}/{len(X_train)} - Min Loss: {str(min_loss)[:5]} - Loss: {epoch_loss}\", end='\\r')\n",
    "    optimiser.alpha *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08cd0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(task_indices):\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    for t in range(len(task_indices)):\n",
    "        input = Tensor([task_indices[t]], autograd=True)\n",
    "        rnn_input = embedding.forward(input=input)\n",
    "        hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "    output = output_layer.forward(hidden[0])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fba0be0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2000/2000 - Loss: [0.13342608]]]5]\n",
      "[33.65812669]\n"
     ]
    }
   ],
   "source": [
    "for task_i in range(len(X_test)):\n",
    "    output = predict(X_test[task_i])\n",
    "    target = Tensor(np.array(y_test)[task_i], autograd=True)\n",
    "    loss = criterion.forward(output, target)\n",
    "    print(f\"Task {task_i + 1}/{len(X_test)} - Loss: {loss.data}\", end='\\r')\n",
    "    total_loss += loss.data / len(X_test[task_i])\n",
    "loss_to_display = np.exp(total_loss / (task_i + 1))\n",
    "print()\n",
    "print(loss_to_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fbbf87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     pkl\u001b[38;5;241m.\u001b[39mdump(word2index, file)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance_embedding.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 9\u001b[0m     pkl\u001b[38;5;241m.\u001b[39mdump(embedding, file)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     12\u001b[0m     pkl\u001b[38;5;241m.\u001b[39mdump(model, file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"importance_embedding.pkl\", \"wb\") as file:\n",
    "    pkl.dump(embedding, file)\n",
    "\n",
    "with open(\"importance_model.pkl\", \"wb\") as file:\n",
    "    pkl.dump(model, file)\n",
    "\n",
    "with open(\"importance_output.pkl\", \"wb\") as file:\n",
    "    pkl.dump(output_layer, file)\n",
    "\n",
    "with open(\"word2index.pkl\", \"wb\") as file:\n",
    "    pkl.dump(word2index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf352be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
